var searchIndex = JSON.parse('{\
"llamapun":{"doc":"The `LLaMaPUn` library in RustLanguage and Mathematics…","i":[[0,"util","llamapun","Various useful code snippets",null,null],[0,"data_helpers","llamapun::util","Helpers with transactional logic related to llamapun::data…",null,null],[3,"LexicalOptions","llamapun::util::data_helpers","Options for lexical normalization on an individual word",null,null],[12,"discard_math","","math will be entirely omitted when set",0,null],[12,"discard_punct","","non-alphanumeric characters will be entirely omitted when…",0,null],[12,"discard_case","","all letters will be lowercased when set",0,null],[5,"ams_normalize_word_range","","Normalization of word lexemes created for the \\\"AMS…",null,[[["lexicaloptions",3],["dnmrange",3],["context",3]],[["result",4],["string",3],["box",3]]]],[5,"heading_from_node_aux","","Provides a string for a given heading node, using…",null,[[["tokenizer",3],["ronode",3],["context",3]],[["option",4],["string",3]]]],[5,"normalize_heading_title","","Attempt to recover the \\\"type\\\" of a potentially specialized…",null,[[],["string",3]]],[5,"invalid_for_english_latin","","Check if the given DNM contains valid English+Latin content",null,[[["dnm",3]]]],[0,"path_helpers","llamapun::util","Helpers intended mostly for non-Rust use, where rust is…",null,null],[5,"path_to_words","llamapun::util::path_helpers","Given a path to a document, return a word-tokenized string…",null,[[["string",3]],["string",3]]],[0,"plot","llamapun::util","Some plotting functionality using gnuplot",null,null],[5,"plot_simple","llamapun::util::plot","A simple plot",null,[[]]],[0,"test","llamapun::util","Test utilities for llamapun\'s crate",null,null],[3,"RESOURCE_DOCUMENTS","llamapun::util::test","shorthand global for all usable documents in the…",null,null],[0,"token_model","llamapun::util","A \\\"corpus token model\\\"-generation utilities",null,null],[5,"extract","llamapun::util::token_model","Parallel traversal of latexml-style HTML5 document…",null,[[["string",3]],[["result",4],["hashmap",3],["box",3]]]],[0,"ams","llamapun","Representation, normalization and utilities for working…",null,null],[4,"StructuralEnv","llamapun::ams","Semantically fixed structural environments in scientific…",null,null],[13,"Abstract","","",1,null],[13,"Acknowledgement","","",1,null],[13,"Analysis","","",1,null],[13,"Application","","",1,null],[13,"Assumption","","",1,null],[13,"Background","","",1,null],[13,"Case","","",1,null],[13,"Caption","","",1,null],[13,"Claim","","",1,null],[13,"Conclusion","","",1,null],[13,"Condition","","",1,null],[13,"Conjecture","","",1,null],[13,"Contribution","","",1,null],[13,"Corollary","","",1,null],[13,"Data","","",1,null],[13,"Dataset","","",1,null],[13,"Definition","","",1,null],[13,"Demonstration","","",1,null],[13,"Description","","",1,null],[13,"Discussion","","",1,null],[13,"Example","","",1,null],[13,"Experiment","","",1,null],[13,"Fact","","",1,null],[13,"FutureWork","","",1,null],[13,"Implementation","","",1,null],[13,"Introduction","","",1,null],[13,"Lemma","","",1,null],[13,"Methods","","",1,null],[13,"Model","","",1,null],[13,"Motivation","","",1,null],[13,"Notation","","",1,null],[13,"Observation","","",1,null],[13,"Other","","",1,null],[13,"Preliminaries","","",1,null],[13,"Problem","","A task to be solved (sometimes with solution following),…",1,null],[13,"Proof","","",1,null],[13,"Property","","",1,null],[13,"Proposition","","",1,null],[13,"Question","","",1,null],[13,"RelatedWork","","",1,null],[13,"Remark","","",1,null],[13,"Result","","",1,null],[13,"Simulation","","",1,null],[13,"Step","","",1,null],[13,"Summary","","",1,null],[13,"Theorem","","",1,null],[13,"Theory","","",1,null],[4,"AmsEnv","","Author-annotated \\\\newthorem{} environments using the…",null,null],[13,"Acknowledgement","","typically co-author support for a proof/paper (also…",2,null],[13,"Algorithm","","usually defines a computer science algorithm (also…",2,null],[13,"Answer","","To be analyzed (?)",2,null],[13,"Affirmation","","To be analyzed (?)",2,null],[13,"Assumption","","assumption/axiom/assertion/prior -- should they be…",2,null],[13,"Bound","","To be analyzed (?)",2,null],[13,"Caption","","usually an actual Figure or Table captions realized via…",2,null],[13,"Case","","A case in a multi-step proof / description / exposition",2,null],[13,"Claim","","To be analyzed (?)",2,null],[13,"Comment","","To be analyzed (?)",2,null],[13,"Conclusion","","To be analyzed (?)",2,null],[13,"Condition","","Potentially a constraint on a proof",2,null],[13,"Conjecture","","An unproven statement/theorem (includes \\\"conjecture\\\",…",2,null],[13,"Constraint","","To be analyzed (?)",2,null],[13,"Convention","","To be analyzed (?)",2,null],[13,"Corollary","","A direct-to-derive consequence of a prior proposition",2,null],[13,"Criterion","","To be analyzed (?)",2,null],[13,"Definition","","Unlike notations, introduces new conceptual mathematical…",2,null],[13,"Demonstration","","To be analyzed (?)",2,null],[13,"Discussion","","To be analyzed (?)",2,null],[13,"Example","","Demonstration of a definition, notation etc (also…",2,null],[13,"Experiment","","To be analyzed (?)",2,null],[13,"Expansion","","To be analyzed (?)",2,null],[13,"Expectation","","To be analyzed (?)",2,null],[13,"Explanation","","To be analyzed (?)",2,null],[13,"Fact","","To be analyzed (?)",2,null],[13,"Hint","","To be analyzed (?)",2,null],[13,"Issue","","To be analyzed (?)",2,null],[13,"Keywords","","To be analyzed (?)",2,null],[13,"Lemma","","A smaller sub-theorem to a main theorem",2,null],[13,"Notation","","Introduces a new syntactic rule, usually for convenience /…",2,null],[13,"Note","","To be analyzed (?)",2,null],[13,"Notice","","To be analyzed (?)",2,null],[13,"Observation","","To be analyzed (?)",2,null],[13,"Paragraph","","A named paragraph, without a clear standalone function",2,null],[13,"Principle","","To be analyzed (?)",2,null],[13,"Problem","","A task to be solved (sometimes with solution following),…",2,null],[13,"Proof","","Proves a prior theorem/lemma",2,null],[13,"Proposition","","A provably true/false statement. Is this a synonym to…",2,null],[13,"Question","","(sometimes) initial goal of inquiry (also \\\"puzzle\\\", \\\"query\\\")",2,null],[13,"Remark","","A comment that is an aside to the main line of reasoning",2,null],[13,"Result","","Summarizes paper\'s experimental deliverables",2,null],[13,"Rule","","To be analyzed (?)",2,null],[13,"Solution","","To be analyzed (?)",2,null],[13,"Step","","A part of a proof, or demonstration/layout",2,null],[13,"Summary","","To be analyzed (?)",2,null],[13,"Theorem","","A main proposition to be proven in the document",2,null],[13,"Other","","Anything else that was marked up with AMS, but doesn\'t fit…",2,null],[5,"has_markup","","Checks a llamapun `Document` for \'ltx_theorem\' AMS markup",null,[[["document",3]]]],[5,"has_markup_xmldoc","","Checks a libxml document for `ltx_theorem` AMS markup",null,[[["xmldoc",3]]]],[5,"class_to_env","","Maps a latexml-produced HTML class, such as \\\"ltx_theorem…",null,[[],[["amsenv",4],["option",4]]]],[5,"normalize_env","","If known, maps a commonly used AMS environment to a…",null,[[],["amsenv",4]]],[0,"data","llamapun","Data structures and Iterators for convenient high-level…",null,null],[3,"Corpus","llamapun::data","An iterable Corpus of HTML5 documents",null,null],[12,"path","","root directory",3,null],[12,"xml_parser","","document XHTML5 parser",3,null],[12,"html_parser","","document HTML5 parser",3,null],[12,"tokenizer","","`DNM`-aware sentence and word tokenizer",3,null],[12,"senna","","`Senna` object for shallow language analysis",3,null],[12,"senna_options","","`Senna` parsing options",3,null],[12,"dnm_parameters","","Default setting for `DNM` generation",3,null],[12,"extension","","Extension of corpus files (for specially tailored…",3,null],[3,"DocumentIterator","","File-system iterator yielding individual documents",null,null],[12,"corpus","","reference to the parent corpus",4,null],[3,"Document","","One of our math documents.",null,null],[12,"dom","","The DOM of the document",5,null],[12,"path","","The file path of the document",5,null],[12,"corpus","","A reference to the corpus containing this document",5,null],[12,"dnm","","If it exists, the DNM corresponding to this document",5,null],[3,"ParagraphIterator","","An iterator over paragraphs of a `Document`. Ignores…",null,null],[12,"document","","A reference to the document over which we iterate",6,null],[3,"Paragraph","","A paragraph of a document with a DNM",null,null],[12,"dnm","","The dnm of this paragraph",7,null],[12,"document","","A reference to the document containing this paragraph",7,null],[3,"SentenceIterator","","An iterator over the sentences of a document/paragraph",null,null],[12,"document","","A reference to the document we are working on",8,null],[3,"Sentence","","A sentence in a document",null,null],[12,"range","","The range of the sentence",9,null],[12,"document","","The document containing this sentence",9,null],[12,"senna_sentence","","If it exists, also the senna version of the sentence,…",9,null],[3,"SimpleWordIterator","","An iterator over the words of a sentence, where the words…",null,null],[12,"sentence","","The sentence containing the words",10,null],[3,"SennaWordIterator","","An iterator over the words of a sentence, where the words…",null,null],[12,"sentence","","The sentence we are iterating over",11,null],[3,"Word","","A word with a POS tag",null,null],[12,"range","","The range of the word",12,null],[12,"sentence","","The sentence containing this word",12,null],[12,"pos","","The part-of-speech tag of the word (or POS::NOT_SET)",12,null],[11,"new","","Create a new corpus with the base directory `dirpath`",3,[[["string",3]]]],[11,"iter","","Get an iterator over the documents",3,[[],["documentiterator",3]]],[11,"load_doc","","Load a specific document in the corpus",3,[[["string",3]],[["document",3],["result",4],["xmlparseerror",4]]]],[11,"new","","Load a new document",5,[[["string",3],["corpus",3]],[["result",4],["xmlparseerror",4]]]],[11,"paragraph_nodes","","Obtain the problem-free logical paragraphs of a libxml…",5,[[["xmldoc",3]],[["vec",3],["ronode",3]]]],[11,"paragraph_iter","","Get an iterator over the paragraphs of the document",5,[[],["paragraphiterator",3]]],[11,"get_math_nodes","","Obtain the MathML  nodes of a libxml `Document`",5,[[],[["vec",3],["ronode",3]]]],[11,"get_ref_nodes","","Obtain the <span[class=ltx_ref]> nodes of a libxml…",5,[[],[["vec",3],["ronode",3]]]],[11,"sentence_iter","","Get an iterator over the sentences of the document",5,[[],["sentenceiterator",3]]],[11,"iter","","Get an iterator over the sentences in this paragraph",7,[[],["sentenceiterator",3]]],[11,"simple_iter","","Get an iterator over the words (using rudimentary…",9,[[],["simpleworditerator",3]]],[11,"senna_iter","","Get an iterator over the words using Senna",9,[[],["sennaworditerator",3]]],[11,"senna_parse","","Parses the sentence using Senna. The parse options are set…",9,[[]]],[0,"dnm","llamapun","The `dnm` can be used for easier switching between the DOM…",null,null],[3,"DNMParameters","llamapun::dnm","Parameters for the DNM generation",null,null],[12,"special_tag_name_options","","How to deal with special tags (e.g. `<math>` tags)",13,null],[12,"special_tag_class_options","","How to deal with tags with special class names (e.g.…",13,null],[12,"normalize_white_spaces","","merge sequences of whitespaces into a single \' \'. Doesn\'t…",13,null],[12,"wrap_tokens","","put spaces before and after tokens",13,null],[12,"normalize_unicode","","Replace unicode characters by the ascii code representation",13,null],[12,"stem_words_once","","Apply the morpha stemmer once to the text nodes",13,null],[12,"stem_words_full","","Apply the morpha stemmer to the text nodes as often as it…",13,null],[12,"convert_to_lowercase","","Move to lowercase (remark: The stemmer does this…",13,null],[12,"support_back_mapping","","Support back mapping, i.e. mapping plaintext offsets back…",13,null],[3,"RuntimeParseData","","Some temporary data for the parser",null,null],[12,"had_whitespace","","plaintext is currently terminated by some whitespace",14,null],[12,"chars","","plaintext representation as vector of chars (to deal with…",14,null],[3,"DNMRange","","Very often we\'ll talk about substrings of the plaintext -…",null,null],[12,"start","","Offset of the beginning of the range",15,null],[12,"end","","Offset of the end of the range",15,null],[12,"dnm","","DNM containing this range",15,null],[3,"DNM","","The `DNM` is essentially a wrapper around the plain text…",null,null],[12,"plaintext","","The plaintext",16,null],[12,"byte_offsets","","As the plaintext is UTF-8: the byte offsets of the…",16,null],[12,"parameters","","The options for generation",16,null],[12,"root_node","","The root node of the underlying xml tree",16,null],[12,"node_map","","Maps nodes to plaintext offsets",16,null],[12,"runtime","","A runtime object used for holding auxiliary state",16,null],[12,"back_map","","maps an offset to the corresponding node, and the offset…",16,null],[4,"SpecialTagsOption","","Specifies how to deal with a certain tag",null,null],[13,"Enter","","Recurse into tag (default behaviour)",17,null],[13,"Normalize","","Normalize tag, replacing it by some token",17,null],[13,"FunctionNormalize","","Normalize tag, obtain replacement string by function call",17,null],[13,"Skip","","Skip tag",17,null],[11,"to_c14n_basic","","Our linguistic canonical form will only include 1) node…",16,[[],["string",3]]],[11,"node_c14n_basic","","Canonicalize a single node of choice",16,[[["ronode",3]],["string",3]]],[11,"to_hash_basic","","Obtain an MD5 hash from the canonical string of the entire…",16,[[],["string",3]]],[11,"node_hash_basic","","Obtain an MD5 hash from the canonical string of a Node",16,[[["ronode",3]],["string",3]]],[0,"node","","Node auxiliaries for DNMs",null,null],[5,"lexematize_math","llamapun::dnm::node","Map math nodes to their lexemes",null,[[["ronode",3],["context",3]],["string",3]]],[11,"llamapun_normalization","llamapun::dnm","Normalize in a reasonable way for our math documents",13,[[],["dnmparameters",3]]],[11,"check","","Prints warnings, if the parameter settings don\'t make…",13,[[]]],[11,"get_plaintext","","Get the plaintext substring corresponding to the range",15,[[]]],[11,"get_plaintext_truncated","","Get the plaintext without trailing white spaces",15,[[]]],[11,"get_node","","Get the first corresponding DOM node for this range",15,[[],["ronode",3]]],[11,"trim","","Returns a `DNMRange` with the leading and trailing…",15,[[],["dnmrange",3]]],[11,"get_subrange","","returns a subrange, with offsets relative to the beginning…",15,[[],["dnmrange",3]]],[11,"get_subrange_from_byte_offsets","","returns a subrange from a pair of byte offsets (not…",15,[[],["dnmrange",3]]],[11,"is_empty","","checks whether the range is empty",15,[[]]],[11,"serialize","","serializes a DNMRange into an XPointer",15,[[],["string",3]]],[11,"create_arange","","creates an arange from to xpointers",15,[[],["string",3]]],[11,"serialize_offset","","Serializes a node and an offset into an xpointer is_end…",15,[[["ronode",3]],["string",3]]],[11,"serialize_node","","serializes a node into an xpath expression",15,[[["ronode",3]],["string",3]]],[11,"deserialize","","deserializes an xpointer into a `DNMRange`. Note that only…",15,[[["dnm",3],["context",3]],["dnmrange",3]]],[11,"new","","Creates a `DNM` for `root`",16,[[["ronode",3],["dnmparameters",3]],["dnm",3]]],[11,"from_str","","Use the DNM abstraction over a plaintext utterance,…",16,[[["dnmparameters",3],["option",4]],[["result",4],["box",3]]]],[11,"from_ams_paragraph_str","","Rebuild a llamapun-generated tokenized plaintext into a…",16,[[["dnmparameters",3],["option",4]],[["result",4],["box",3]]]],[11,"get_range_of_node","","Get the plaintext range of a node",16,[[["ronode",3]],[["box",3],["dnmrange",3],["result",4]]]],[11,"get_range","","Get the range representing the full DNM",16,[[],[["box",3],["dnmrange",3],["result",4]]]],[11,"get_plaintext","","Get the underlying text for this DNM",16,[[]]],[0,"ngrams","llamapun","A small ngram library ngrams are sequences of n…",null,null],[3,"Dictionary","llamapun::ngrams","Records single words, in order of appearance",null,null],[12,"map","","hashmap for the records",18,null],[3,"Ngrams","","Ngrams are dictionaries with",null,null],[12,"anchor","","anchor word that must be present in all ngram contexts (in…",19,null],[12,"window_size","","if an anchor word is given, word window size, applied to…",19,null],[12,"n","","n-grams for a sequence of n words",19,null],[12,"counts","","statistics hashmap for the occurence counts",19,null],[11,"new","","create a new dictionary",18,[[]]],[11,"insert","","insert a new word into the dictionary (if it hasn\'t been…",18,[[["string",3]]]],[11,"sorted","","get the entries of the dictionary sorted by occurence",18,[[],["vec",3]]],[11,"count","","get the number of entries in the dictionary",18,[[]]],[11,"get","","Get the word count",19,[[]]],[11,"insert","","count a newly seen ngram phrase",19,[[["string",3]]]],[11,"sorted","","obtain the ngram report, sorted by descending frequency",19,[[],["vec",3]]],[11,"distinct_count","","get the number of distinct ngrams recorded",19,[[]]],[11,"add_content","","add content for ngram analysis, typically a paragraph or a…",19,[[]]],[11,"add_anchored_content","","In essence, for a given window size W, a word at index i…",19,[[]]],[11,"record_words","","Take an arbitrarily long vector of words, and record all…",19,[[["vec",3]]]],[0,"parallel_data","llamapun","Data structures and Iterators for rayon-enabled parallel…",null,null],[3,"ItemDNM","llamapun::parallel_data","A DNM with associated document parent (e.g. for…",null,null],[12,"dnm","","The payload of the item",20,null],[12,"document","","A reference to the parent document",20,null],[3,"ItemDNMRange","","A DNMRange with associated document",null,null],[12,"range","","The range of the sentence",21,null],[12,"document","","The document containing this sentence",21,null],[3,"RoNodeIterator","","Generic iterater over read-only xml nodes. It is the…",null,null],[12,"document","","A reference to the owner document",22,null],[3,"DNMRangeIterator","","A generic iterator over DNMRanges with their associated…",null,null],[12,"document","","A reference to the document we are working on",23,null],[0,"corpus","","container and API for a Corpus capable of parallel walks…",null,null],[3,"Corpus","llamapun::parallel_data::corpus","A parallel iterable Corpus of HTML5 documents",null,null],[12,"path","","root directory",24,null],[12,"xml_parser","","document XHTML5 parser",24,null],[12,"html_parser","","document HTML5 parser",24,null],[12,"tokenizer","","`DNM`-aware sentence and word tokenizer",24,null],[12,"dnm_parameters","","Default setting for `DNM` generation",24,null],[12,"extension","","Extension of corpus files (for specially tailored…",24,null],[11,"new","","Create a new parallel-processing corpus with the base…",24,[[["string",3]]]],[11,"catalog_with_parallel_walk","","Get a parallel iterator over the documents",24,[[],[["hashmap",3],["string",3]]]],[0,"document","llamapun::parallel_data","container and API for a Document yielded during a parallel…",null,null],[3,"Document","llamapun::parallel_data::document","One of our math documents, thread-friendly",null,null],[12,"dom","","The DOM of the document",25,null],[12,"path","","The file path of the document",25,null],[12,"corpus","","A reference to the corpus containing this document",25,null],[12,"dnm","","If it exists, the DNM corresponding to this document",25,null],[11,"new","","Load a new document",25,[[["string",3],["corpus",3]],[["result",4],["xmlparseerror",4]]]],[11,"get_heading_nodes","","Obtain the problem-free logical headings of a libxml…",25,[[],[["vec",3],["ronode",3]]]],[11,"heading_iter","","Get an iterator over the headings of the document",25,[[],["ronodeiterator",3]]],[11,"get_paragraph_nodes","","Obtain the problem-free logical paragraphs of a libxml…",25,[[],[["vec",3],["ronode",3]]]],[11,"paragraph_iter","","Get an iterator over the paragraphs of the document",25,[[],["ronodeiterator",3]]],[11,"extended_paragraph_iter","","Get an iterator over textual paragraphs of the document,…",25,[[],["ronodeiterator",3]]],[11,"get_math_nodes","","Obtain the MathML  nodes of a libxml `Document`",25,[[],[["vec",3],["ronode",3]]]],[11,"get_ref_nodes","","Obtain the <span[class=ltx_ref]> nodes of a libxml…",25,[[],[["vec",3],["ronode",3]]]],[11,"sentence_iter","","Get an iterator over the sentences of the document",25,[[],["dnmrangeiterator",3]]],[11,"get_xpath_nodes","","Obtain the nodes associated with the xpath evaluation over…",25,[[],[["vec",3],["ronode",3]]]],[11,"get_xpath_node","","Obtain the first node associated with the xpath evaluation…",25,[[],[["option",4],["ronode",3]]]],[11,"xpath_selector_iter","","Get an iterator over a custom xpath selector over the…",25,[[],["ronodeiterator",3]]],[11,"filter_iter","","Get an iterator using a custom closure predicate filter…",25,[[["fn",8]],["ronodeiterator",3]]],[8,"XPathFilteredIterator","llamapun::parallel_data","An iterator adaptor for filtered selections over a document",null,null],[10,"to_sentences","","the sentences for the resulting selection",26,[[],[["dnmrange",3],["vec",3]]]],[10,"get_document","","the owner document being selected over",26,[[],["document",3]]],[11,"iter","","Get an iterator over the sentences in this paragraph",26,[[],["dnmrangeiterator",3]]],[11,"word_iter","","Get an iterator over the words (using rudimentary…",21,[[],["dnmrangeiterator",3]]],[11,"word_and_punct_iter","","Get an iterator over the words and punctuation (using…",21,[[],["dnmrangeiterator",3]]],[11,"word_iter","","Get an iterator over the words (using rudimentary…",20,[[],["dnmrangeiterator",3]]],[11,"word_and_punct_iter","","Get an iterator over the words and punctuation (using…",20,[[],["dnmrangeiterator",3]]],[0,"patterns","llamapun","A module for pattern matching in mathematical documents",null,null],[3,"Match","llamapun::patterns","A `Match`. Note that matches are represented in a tree…",null,null],[12,"marker","","The marker associated with this match",27,null],[12,"sub_matches","","The sub-matches",27,null],[3,"MathMarker","","A marked math node",null,null],[12,"node","","the marked math node",28,null],[12,"marker","","the marker",28,null],[3,"PatternFile","","Contains rules loaded from a pattern file",null,null],[12,"description","","description of the file",29,null],[12,"word_rules","","the word rules",29,null],[12,"pos_rules","","the POS rules",29,null],[12,"math_rules","","the math rules",29,null],[12,"mtext_rules","","the mtext rules (math symbols)",29,null],[12,"sequence_rules","","the sequence rules",29,null],[12,"word_rule_names","","matches names of word rules to their offsets",29,null],[12,"pos_rule_names","","matches names of POS rules to their offsets",29,null],[12,"math_rule_names","","matches names of math rules to their offsets",29,null],[12,"mtext_rule_names","","matches names of mtext rules to their offsets",29,null],[12,"sequence_rule_names","","matches names of sequence rules to their offsets",29,null],[3,"PatternMarker","","The marker used for marking patterns. If a match was…",null,null],[12,"name","","name of the marker",30,null],[12,"tags","","tags of the marker",30,null],[3,"TextMarker","","A marked text range",null,null],[12,"range","","the marked range",31,null],[12,"marker","","the marker",31,null],[4,"MarkerEnum","","Any marked result",null,null],[13,"Text","","a marked text range",32,null],[13,"Math","","a marked math node",32,null],[5,"match_sentence","","returns the matches in a sentence",null,[[["sentence",3],["dnmrange",3],["patternfile",3]],[["result",4],["vec",3],["string",3]]]],[11,"get_marker_list","","returns a list of all markers",27,[[],[["vec",3],["markerenum",4]]]],[11,"load","","loads a pattern file",29,[[],[["string",3],["result",4],["patternfile",3]]]],[0,"stopwords","llamapun","A tiny stopwords library Stopwords are words frequent…",null,null],[5,"load","llamapun::stopwords","Load a set of stopwords Annoyingly, `HashSet`s are not…",null,[[],["hashset",3]]],[0,"tokenizer","llamapun","Provides functionality for tokenizing sentences and words",null,null],[3,"Tokenizer","llamapun::tokenizer","Stores auxiliary resources required by the tokenizer so…",null,null],[12,"stopwords","","set of stopwords",33,null],[12,"abbreviations","","regular expression for abbreviations",33,null],[11,"sentences","","gets the sentences from a dnm",33,[[["dnm",3]],[["vec",3],["dnmrange",3]]]],[11,"words","","returns the words of a sentence using simple heuristics",33,[[["dnmrange",3]],[["vec",3],["dnmrange",3]]]],[11,"words_and_punct","","returns the words and punctuation of a sentence, using…",33,[[["dnmrange",3]],[["vec",3],["dnmrange",3]]]],[0,"extern_use","llamapun","Expose convenience calls to be used from non-Rust…",null,null],[5,"word_tokenize_for_vec2doc","llamapun::extern_use","Interface function for vec2doc-expected word tokenization…",null,[[]]],[14,"record_node_map","llamapun","A handy macro for idiomatic recording in the node_map",null,null],[11,"from","llamapun::util::data_helpers","",0,[[]]],[11,"into","","",0,[[]]],[11,"borrow","","",0,[[]]],[11,"borrow_mut","","",0,[[]]],[11,"try_from","","",0,[[],["result",4]]],[11,"try_into","","",0,[[],["result",4]]],[11,"type_id","","",0,[[],["typeid",3]]],[11,"init","","",0,[[]]],[11,"deref","","",0,[[]]],[11,"deref_mut","","",0,[[]]],[11,"drop","","",0,[[]]],[11,"from","llamapun::util::test","",34,[[]]],[11,"into","","",34,[[]]],[11,"borrow","","",34,[[]]],[11,"borrow_mut","","",34,[[]]],[11,"try_from","","",34,[[],["result",4]]],[11,"try_into","","",34,[[],["result",4]]],[11,"type_id","","",34,[[],["typeid",3]]],[11,"init","","",34,[[]]],[11,"deref","","",34,[[]]],[11,"deref_mut","","",34,[[]]],[11,"drop","","",34,[[]]],[11,"from","llamapun::ams","",1,[[]]],[11,"into","","",1,[[]]],[11,"to_owned","","",1,[[]]],[11,"clone_into","","",1,[[]]],[11,"to_string","","",1,[[],["string",3]]],[11,"borrow","","",1,[[]]],[11,"borrow_mut","","",1,[[]]],[11,"try_from","","",1,[[],["result",4]]],[11,"try_into","","",1,[[],["result",4]]],[11,"type_id","","",1,[[],["typeid",3]]],[11,"init","","",1,[[]]],[11,"deref","","",1,[[]]],[11,"deref_mut","","",1,[[]]],[11,"drop","","",1,[[]]],[11,"from","","",2,[[]]],[11,"into","","",2,[[]]],[11,"to_owned","","",2,[[]]],[11,"clone_into","","",2,[[]]],[11,"to_string","","",2,[[],["string",3]]],[11,"borrow","","",2,[[]]],[11,"borrow_mut","","",2,[[]]],[11,"try_from","","",2,[[],["result",4]]],[11,"try_into","","",2,[[],["result",4]]],[11,"type_id","","",2,[[],["typeid",3]]],[11,"init","","",2,[[]]],[11,"deref","","",2,[[]]],[11,"deref_mut","","",2,[[]]],[11,"drop","","",2,[[]]],[11,"from","llamapun::data","",3,[[]]],[11,"into","","",3,[[]]],[11,"borrow","","",3,[[]]],[11,"borrow_mut","","",3,[[]]],[11,"try_from","","",3,[[],["result",4]]],[11,"try_into","","",3,[[],["result",4]]],[11,"type_id","","",3,[[],["typeid",3]]],[11,"init","","",3,[[]]],[11,"deref","","",3,[[]]],[11,"deref_mut","","",3,[[]]],[11,"drop","","",3,[[]]],[11,"from","","",4,[[]]],[11,"into","","",4,[[]]],[11,"into_iter","","",4,[[]]],[11,"borrow","","",4,[[]]],[11,"borrow_mut","","",4,[[]]],[11,"try_from","","",4,[[],["result",4]]],[11,"try_into","","",4,[[],["result",4]]],[11,"type_id","","",4,[[],["typeid",3]]],[11,"init","","",4,[[]]],[11,"deref","","",4,[[]]],[11,"deref_mut","","",4,[[]]],[11,"drop","","",4,[[]]],[11,"from","","",5,[[]]],[11,"into","","",5,[[]]],[11,"borrow","","",5,[[]]],[11,"borrow_mut","","",5,[[]]],[11,"try_from","","",5,[[],["result",4]]],[11,"try_into","","",5,[[],["result",4]]],[11,"type_id","","",5,[[],["typeid",3]]],[11,"init","","",5,[[]]],[11,"deref","","",5,[[]]],[11,"deref_mut","","",5,[[]]],[11,"drop","","",5,[[]]],[11,"from","","",6,[[]]],[11,"into","","",6,[[]]],[11,"into_iter","","",6,[[]]],[11,"borrow","","",6,[[]]],[11,"borrow_mut","","",6,[[]]],[11,"try_from","","",6,[[],["result",4]]],[11,"try_into","","",6,[[],["result",4]]],[11,"type_id","","",6,[[],["typeid",3]]],[11,"init","","",6,[[]]],[11,"deref","","",6,[[]]],[11,"deref_mut","","",6,[[]]],[11,"drop","","",6,[[]]],[11,"from","","",7,[[]]],[11,"into","","",7,[[]]],[11,"borrow","","",7,[[]]],[11,"borrow_mut","","",7,[[]]],[11,"try_from","","",7,[[],["result",4]]],[11,"try_into","","",7,[[],["result",4]]],[11,"type_id","","",7,[[],["typeid",3]]],[11,"init","","",7,[[]]],[11,"deref","","",7,[[]]],[11,"deref_mut","","",7,[[]]],[11,"drop","","",7,[[]]],[11,"from","","",8,[[]]],[11,"into","","",8,[[]]],[11,"into_iter","","",8,[[]]],[11,"borrow","","",8,[[]]],[11,"borrow_mut","","",8,[[]]],[11,"try_from","","",8,[[],["result",4]]],[11,"try_into","","",8,[[],["result",4]]],[11,"type_id","","",8,[[],["typeid",3]]],[11,"init","","",8,[[]]],[11,"deref","","",8,[[]]],[11,"deref_mut","","",8,[[]]],[11,"drop","","",8,[[]]],[11,"from","","",9,[[]]],[11,"into","","",9,[[]]],[11,"borrow","","",9,[[]]],[11,"borrow_mut","","",9,[[]]],[11,"try_from","","",9,[[],["result",4]]],[11,"try_into","","",9,[[],["result",4]]],[11,"type_id","","",9,[[],["typeid",3]]],[11,"init","","",9,[[]]],[11,"deref","","",9,[[]]],[11,"deref_mut","","",9,[[]]],[11,"drop","","",9,[[]]],[11,"from","","",10,[[]]],[11,"into","","",10,[[]]],[11,"into_iter","","",10,[[]]],[11,"borrow","","",10,[[]]],[11,"borrow_mut","","",10,[[]]],[11,"try_from","","",10,[[],["result",4]]],[11,"try_into","","",10,[[],["result",4]]],[11,"type_id","","",10,[[],["typeid",3]]],[11,"init","","",10,[[]]],[11,"deref","","",10,[[]]],[11,"deref_mut","","",10,[[]]],[11,"drop","","",10,[[]]],[11,"from","","",11,[[]]],[11,"into","","",11,[[]]],[11,"into_iter","","",11,[[]]],[11,"borrow","","",11,[[]]],[11,"borrow_mut","","",11,[[]]],[11,"try_from","","",11,[[],["result",4]]],[11,"try_into","","",11,[[],["result",4]]],[11,"type_id","","",11,[[],["typeid",3]]],[11,"init","","",11,[[]]],[11,"deref","","",11,[[]]],[11,"deref_mut","","",11,[[]]],[11,"drop","","",11,[[]]],[11,"from","","",12,[[]]],[11,"into","","",12,[[]]],[11,"borrow","","",12,[[]]],[11,"borrow_mut","","",12,[[]]],[11,"try_from","","",12,[[],["result",4]]],[11,"try_into","","",12,[[],["result",4]]],[11,"type_id","","",12,[[],["typeid",3]]],[11,"init","","",12,[[]]],[11,"deref","","",12,[[]]],[11,"deref_mut","","",12,[[]]],[11,"drop","","",12,[[]]],[11,"from","llamapun::dnm","",13,[[]]],[11,"into","","",13,[[]]],[11,"to_owned","","",13,[[]]],[11,"clone_into","","",13,[[]]],[11,"borrow","","",13,[[]]],[11,"borrow_mut","","",13,[[]]],[11,"try_from","","",13,[[],["result",4]]],[11,"try_into","","",13,[[],["result",4]]],[11,"type_id","","",13,[[],["typeid",3]]],[11,"init","","",13,[[]]],[11,"deref","","",13,[[]]],[11,"deref_mut","","",13,[[]]],[11,"drop","","",13,[[]]],[11,"from","","",14,[[]]],[11,"into","","",14,[[]]],[11,"borrow","","",14,[[]]],[11,"borrow_mut","","",14,[[]]],[11,"try_from","","",14,[[],["result",4]]],[11,"try_into","","",14,[[],["result",4]]],[11,"type_id","","",14,[[],["typeid",3]]],[11,"init","","",14,[[]]],[11,"deref","","",14,[[]]],[11,"deref_mut","","",14,[[]]],[11,"drop","","",14,[[]]],[11,"from","","",15,[[]]],[11,"into","","",15,[[]]],[11,"to_owned","","",15,[[]]],[11,"clone_into","","",15,[[]]],[11,"borrow","","",15,[[]]],[11,"borrow_mut","","",15,[[]]],[11,"try_from","","",15,[[],["result",4]]],[11,"try_into","","",15,[[],["result",4]]],[11,"type_id","","",15,[[],["typeid",3]]],[11,"init","","",15,[[]]],[11,"deref","","",15,[[]]],[11,"deref_mut","","",15,[[]]],[11,"drop","","",15,[[]]],[11,"from","","",16,[[]]],[11,"into","","",16,[[]]],[11,"borrow","","",16,[[]]],[11,"borrow_mut","","",16,[[]]],[11,"try_from","","",16,[[],["result",4]]],[11,"try_into","","",16,[[],["result",4]]],[11,"type_id","","",16,[[],["typeid",3]]],[11,"init","","",16,[[]]],[11,"deref","","",16,[[]]],[11,"deref_mut","","",16,[[]]],[11,"drop","","",16,[[]]],[11,"from","","",17,[[]]],[11,"into","","",17,[[]]],[11,"to_owned","","",17,[[]]],[11,"clone_into","","",17,[[]]],[11,"borrow","","",17,[[]]],[11,"borrow_mut","","",17,[[]]],[11,"try_from","","",17,[[],["result",4]]],[11,"try_into","","",17,[[],["result",4]]],[11,"type_id","","",17,[[],["typeid",3]]],[11,"init","","",17,[[]]],[11,"deref","","",17,[[]]],[11,"deref_mut","","",17,[[]]],[11,"drop","","",17,[[]]],[11,"from","llamapun::ngrams","",18,[[]]],[11,"into","","",18,[[]]],[11,"borrow","","",18,[[]]],[11,"borrow_mut","","",18,[[]]],[11,"try_from","","",18,[[],["result",4]]],[11,"try_into","","",18,[[],["result",4]]],[11,"type_id","","",18,[[],["typeid",3]]],[11,"init","","",18,[[]]],[11,"deref","","",18,[[]]],[11,"deref_mut","","",18,[[]]],[11,"drop","","",18,[[]]],[11,"from","","",19,[[]]],[11,"into","","",19,[[]]],[11,"borrow","","",19,[[]]],[11,"borrow_mut","","",19,[[]]],[11,"try_from","","",19,[[],["result",4]]],[11,"try_into","","",19,[[],["result",4]]],[11,"type_id","","",19,[[],["typeid",3]]],[11,"init","","",19,[[]]],[11,"deref","","",19,[[]]],[11,"deref_mut","","",19,[[]]],[11,"drop","","",19,[[]]],[11,"from","llamapun::parallel_data","",20,[[]]],[11,"into","","",20,[[]]],[11,"borrow","","",20,[[]]],[11,"borrow_mut","","",20,[[]]],[11,"try_from","","",20,[[],["result",4]]],[11,"try_into","","",20,[[],["result",4]]],[11,"type_id","","",20,[[],["typeid",3]]],[11,"init","","",20,[[]]],[11,"deref","","",20,[[]]],[11,"deref_mut","","",20,[[]]],[11,"drop","","",20,[[]]],[11,"from","","",21,[[]]],[11,"into","","",21,[[]]],[11,"borrow","","",21,[[]]],[11,"borrow_mut","","",21,[[]]],[11,"try_from","","",21,[[],["result",4]]],[11,"try_into","","",21,[[],["result",4]]],[11,"type_id","","",21,[[],["typeid",3]]],[11,"init","","",21,[[]]],[11,"deref","","",21,[[]]],[11,"deref_mut","","",21,[[]]],[11,"drop","","",21,[[]]],[11,"from","","",22,[[]]],[11,"into","","",22,[[]]],[11,"into_iter","","",22,[[]]],[11,"borrow","","",22,[[]]],[11,"borrow_mut","","",22,[[]]],[11,"try_from","","",22,[[],["result",4]]],[11,"try_into","","",22,[[],["result",4]]],[11,"type_id","","",22,[[],["typeid",3]]],[11,"init","","",22,[[]]],[11,"deref","","",22,[[]]],[11,"deref_mut","","",22,[[]]],[11,"drop","","",22,[[]]],[11,"from","","",23,[[]]],[11,"into","","",23,[[]]],[11,"into_iter","","",23,[[]]],[11,"borrow","","",23,[[]]],[11,"borrow_mut","","",23,[[]]],[11,"try_from","","",23,[[],["result",4]]],[11,"try_into","","",23,[[],["result",4]]],[11,"type_id","","",23,[[],["typeid",3]]],[11,"init","","",23,[[]]],[11,"deref","","",23,[[]]],[11,"deref_mut","","",23,[[]]],[11,"drop","","",23,[[]]],[11,"from","llamapun::parallel_data::corpus","",24,[[]]],[11,"into","","",24,[[]]],[11,"borrow","","",24,[[]]],[11,"borrow_mut","","",24,[[]]],[11,"try_from","","",24,[[],["result",4]]],[11,"try_into","","",24,[[],["result",4]]],[11,"type_id","","",24,[[],["typeid",3]]],[11,"init","","",24,[[]]],[11,"deref","","",24,[[]]],[11,"deref_mut","","",24,[[]]],[11,"drop","","",24,[[]]],[11,"from","llamapun::parallel_data::document","",25,[[]]],[11,"into","","",25,[[]]],[11,"borrow","","",25,[[]]],[11,"borrow_mut","","",25,[[]]],[11,"try_from","","",25,[[],["result",4]]],[11,"try_into","","",25,[[],["result",4]]],[11,"type_id","","",25,[[],["typeid",3]]],[11,"init","","",25,[[]]],[11,"deref","","",25,[[]]],[11,"deref_mut","","",25,[[]]],[11,"drop","","",25,[[]]],[11,"from","llamapun::patterns","",27,[[]]],[11,"into","","",27,[[]]],[11,"to_owned","","",27,[[]]],[11,"clone_into","","",27,[[]]],[11,"borrow","","",27,[[]]],[11,"borrow_mut","","",27,[[]]],[11,"try_from","","",27,[[],["result",4]]],[11,"try_into","","",27,[[],["result",4]]],[11,"type_id","","",27,[[],["typeid",3]]],[11,"init","","",27,[[]]],[11,"deref","","",27,[[]]],[11,"deref_mut","","",27,[[]]],[11,"drop","","",27,[[]]],[11,"from","","",28,[[]]],[11,"into","","",28,[[]]],[11,"to_owned","","",28,[[]]],[11,"clone_into","","",28,[[]]],[11,"borrow","","",28,[[]]],[11,"borrow_mut","","",28,[[]]],[11,"try_from","","",28,[[],["result",4]]],[11,"try_into","","",28,[[],["result",4]]],[11,"type_id","","",28,[[],["typeid",3]]],[11,"init","","",28,[[]]],[11,"deref","","",28,[[]]],[11,"deref_mut","","",28,[[]]],[11,"drop","","",28,[[]]],[11,"from","","",29,[[]]],[11,"into","","",29,[[]]],[11,"borrow","","",29,[[]]],[11,"borrow_mut","","",29,[[]]],[11,"try_from","","",29,[[],["result",4]]],[11,"try_into","","",29,[[],["result",4]]],[11,"type_id","","",29,[[],["typeid",3]]],[11,"init","","",29,[[]]],[11,"deref","","",29,[[]]],[11,"deref_mut","","",29,[[]]],[11,"drop","","",29,[[]]],[11,"from","","",30,[[]]],[11,"into","","",30,[[]]],[11,"to_owned","","",30,[[]]],[11,"clone_into","","",30,[[]]],[11,"borrow","","",30,[[]]],[11,"borrow_mut","","",30,[[]]],[11,"try_from","","",30,[[],["result",4]]],[11,"try_into","","",30,[[],["result",4]]],[11,"type_id","","",30,[[],["typeid",3]]],[11,"init","","",30,[[]]],[11,"deref","","",30,[[]]],[11,"deref_mut","","",30,[[]]],[11,"drop","","",30,[[]]],[11,"from","","",31,[[]]],[11,"into","","",31,[[]]],[11,"to_owned","","",31,[[]]],[11,"clone_into","","",31,[[]]],[11,"borrow","","",31,[[]]],[11,"borrow_mut","","",31,[[]]],[11,"try_from","","",31,[[],["result",4]]],[11,"try_into","","",31,[[],["result",4]]],[11,"type_id","","",31,[[],["typeid",3]]],[11,"init","","",31,[[]]],[11,"deref","","",31,[[]]],[11,"deref_mut","","",31,[[]]],[11,"drop","","",31,[[]]],[11,"from","","",32,[[]]],[11,"into","","",32,[[]]],[11,"to_owned","","",32,[[]]],[11,"clone_into","","",32,[[]]],[11,"borrow","","",32,[[]]],[11,"borrow_mut","","",32,[[]]],[11,"try_from","","",32,[[],["result",4]]],[11,"try_into","","",32,[[],["result",4]]],[11,"type_id","","",32,[[],["typeid",3]]],[11,"init","","",32,[[]]],[11,"deref","","",32,[[]]],[11,"deref_mut","","",32,[[]]],[11,"drop","","",32,[[]]],[11,"from","llamapun::tokenizer","",33,[[]]],[11,"into","","",33,[[]]],[11,"borrow","","",33,[[]]],[11,"borrow_mut","","",33,[[]]],[11,"try_from","","",33,[[],["result",4]]],[11,"try_into","","",33,[[],["result",4]]],[11,"type_id","","",33,[[],["typeid",3]]],[11,"init","","",33,[[]]],[11,"deref","","",33,[[]]],[11,"deref_mut","","",33,[[]]],[11,"drop","","",33,[[]]],[11,"get_document","llamapun::parallel_data","",20,[[],["document",3]]],[11,"to_sentences","","",20,[[],[["dnmrange",3],["vec",3]]]],[11,"from","llamapun::ams","",1,[[],["structuralenv",4]]],[11,"next","llamapun::data","",4,[[],[["document",3],["option",4]]]],[11,"next","","",6,[[],[["option",4],["paragraph",3]]]],[11,"next","","",8,[[],[["option",4],["sentence",3]]]],[11,"next","","",10,[[],[["option",4],["word",3]]]],[11,"next","","",11,[[],[["option",4],["word",3]]]],[11,"next","llamapun::parallel_data","",22,[[],[["option",4],["itemdnm",3]]]],[11,"next","","",23,[[],[["option",4],["itemdnmrange",3]]]],[11,"clone","llamapun::ams","",1,[[],["structuralenv",4]]],[11,"clone","","",2,[[],["amsenv",4]]],[11,"clone","llamapun::dnm","",17,[[],["specialtagsoption",4]]],[11,"clone","","",13,[[],["dnmparameters",3]]],[11,"clone","","",15,[[],["dnmrange",3]]],[11,"clone","llamapun::patterns","",27,[[],["match",3]]],[11,"clone","","",30,[[],["patternmarker",3]]],[11,"clone","","",28,[[],["mathmarker",3]]],[11,"clone","","",31,[[],["textmarker",3]]],[11,"clone","","",32,[[],["markerenum",4]]],[11,"default","llamapun::util::data_helpers","",0,[[]]],[11,"default","llamapun::data","",3,[[],["corpus",3]]],[11,"default","llamapun::dnm","",14,[[],["runtimeparsedata",3]]],[11,"default","","Don\'t do anything fancy and specific by default",13,[[],["dnmparameters",3]]],[11,"default","","",16,[[],["dnm",3]]],[11,"default","llamapun::ngrams","",18,[[],["dictionary",3]]],[11,"default","","",19,[[],["ngrams",3]]],[11,"default","llamapun::parallel_data::corpus","",24,[[],["corpus",3]]],[11,"default","llamapun::tokenizer","",33,[[],["tokenizer",3]]],[11,"eq","llamapun::ams","",1,[[["structuralenv",4]]]],[11,"eq","","",2,[[["amsenv",4]]]],[11,"deref","llamapun::util::test","",34,[[],["vec",3]]],[11,"fmt","llamapun::ams","",1,[[["formatter",3]],["result",6]]],[11,"fmt","","",2,[[["formatter",3]],["result",6]]],[11,"fmt","llamapun::dnm","",14,[[["formatter",3]],["result",6]]],[11,"fmt","","",17,[[["formatter",3]],["result",6]]],[11,"fmt","","",13,[[["formatter",3]],["result",6]]],[11,"fmt","","",15,[[["formatter",3]],["result",6]]],[11,"fmt","","",16,[[["formatter",3]],["result",6]]],[11,"fmt","llamapun::ams","",1,[[["formatter",3]],["result",6]]],[11,"fmt","","",2,[[["formatter",3]],["result",6]]],[11,"initialize","llamapun::util::test","",34,[[]]]],"p":[[3,"LexicalOptions"],[4,"StructuralEnv"],[4,"AmsEnv"],[3,"Corpus"],[3,"DocumentIterator"],[3,"Document"],[3,"ParagraphIterator"],[3,"Paragraph"],[3,"SentenceIterator"],[3,"Sentence"],[3,"SimpleWordIterator"],[3,"SennaWordIterator"],[3,"Word"],[3,"DNMParameters"],[3,"RuntimeParseData"],[3,"DNMRange"],[3,"DNM"],[4,"SpecialTagsOption"],[3,"Dictionary"],[3,"Ngrams"],[3,"ItemDNM"],[3,"ItemDNMRange"],[3,"RoNodeIterator"],[3,"DNMRangeIterator"],[3,"Corpus"],[3,"Document"],[8,"XPathFilteredIterator"],[3,"Match"],[3,"MathMarker"],[3,"PatternFile"],[3,"PatternMarker"],[3,"TextMarker"],[4,"MarkerEnum"],[3,"Tokenizer"],[3,"RESOURCE_DOCUMENTS"]]}\
}');
addSearchOptions(searchIndex);initSearch(searchIndex);