var searchIndex = JSON.parse('{\
"llamapun":{"doc":"The <code>LLaMaPUn</code> library in Rust","t":[0,0,0,0,0,0,0,14,0,0,0,13,13,13,13,13,4,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,4,13,13,13,13,13,11,11,11,11,5,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,5,5,11,11,11,11,5,11,11,11,11,11,11,11,11,11,11,3,3,3,3,3,3,3,3,3,3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,12,11,11,11,11,11,11,11,11,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,11,12,11,12,12,12,12,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,3,3,3,13,13,13,3,13,4,12,11,11,11,11,11,11,11,11,11,11,12,12,11,11,11,11,11,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,11,11,11,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,11,0,11,11,12,12,12,12,12,12,12,11,11,11,12,12,12,12,12,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,5,5,3,3,11,11,12,11,11,11,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,11,11,11,11,11,11,11,11,11,11,12,3,3,3,3,8,11,11,11,11,11,11,11,11,0,11,11,11,11,11,11,11,11,12,0,12,12,12,12,11,11,11,11,11,11,11,11,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,3,11,11,11,11,11,11,12,11,12,11,12,11,11,11,12,12,11,11,11,12,3,11,11,12,11,11,12,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,11,11,11,11,11,4,3,13,3,3,3,13,3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,5,12,12,12,12,12,12,12,12,12,12,12,12,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,12,5,3,12,11,11,11,11,11,11,11,11,11,11,12,11,11,11,11,11,0,0,0,0,0,3,5,11,11,11,11,11,12,12,12,11,11,5,11,11,5,5,11,11,11,5,5,3,11,11,11,11,11,11,11,11,11,11,11,11,5],"n":["ams","data","dnm","extern_use","ngrams","parallel_data","patterns","record_node_map","stopwords","tokenizer","util","Abstract","Acknowledgement","Acknowledgement","Affirmation","Algorithm","AmsEnv","Analysis","Answer","Application","Assumption","Assumption","Background","Bound","Caption","Caption","Case","Case","Claim","Claim","Comment","Conclusion","Conclusion","Condition","Condition","Conjecture","Conjecture","Constraint","Contribution","Convention","Corollary","Corollary","Criterion","Data","Dataset","Definition","Definition","Demonstration","Demonstration","Description","Discussion","Discussion","Example","Example","Expansion","Expectation","Experiment","Experiment","Explanation","Fact","Fact","FutureWork","Hint","Implementation","Introduction","Issue","Keywords","Lemma","Lemma","Methods","Model","Motivation","Notation","Notation","Note","Notice","Observation","Observation","Other","Other","Paragraph","Preliminaries","Principle","Problem","Problem","Proof","Proof","Property","Proposition","Proposition","Question","Question","RelatedWork","Remark","Remark","Result","Result","Rule","Simulation","Solution","Step","Step","StructuralEnv","Summary","Summary","Theorem","Theorem","Theory","borrow","borrow","borrow_mut","borrow_mut","class_to_env","clone","clone","clone_into","clone_into","deref","deref","deref_mut","deref_mut","drop","drop","eq","eq","fmt","fmt","fmt","fmt","from","from","from","has_markup","has_markup_xmldoc","init","init","into","into","normalize_env","to_owned","to_owned","to_string","to_string","try_from","try_from","try_into","try_into","type_id","type_id","Corpus","Document","DocumentIterator","Paragraph","ParagraphIterator","SennaWordIterator","Sentence","SentenceIterator","SimpleWordIterator","Word","borrow","borrow","borrow","borrow","borrow","borrow","borrow","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","corpus","corpus","default","deref","deref","deref","deref","deref","deref","deref","deref","deref","deref","deref_mut","deref_mut","deref_mut","deref_mut","deref_mut","deref_mut","deref_mut","deref_mut","deref_mut","deref_mut","dnm","dnm","dnm_parameters","document","document","document","document","dom","drop","drop","drop","drop","drop","drop","drop","drop","drop","drop","extension","from","from","from","from","from","from","from","from","from","from","get_math_nodes","get_ref_nodes","html_parser","init","init","init","init","init","init","init","init","init","init","into","into","into","into","into","into","into","into","into","into","into_iter","into_iter","into_iter","into_iter","into_iter","iter","iter","load_doc","new","new","next","next","next","next","next","paragraph_iter","paragraph_nodes","path","path","pos","range","range","senna","senna_iter","senna_options","senna_parse","senna_sentence","sentence","sentence","sentence","sentence_iter","simple_iter","tokenizer","try_from","try_from","try_from","try_from","try_from","try_from","try_from","try_from","try_from","try_from","try_into","try_into","try_into","try_into","try_into","try_into","try_into","try_into","try_into","try_into","type_id","type_id","type_id","type_id","type_id","type_id","type_id","type_id","type_id","type_id","xml_parser","DNM","DNMParameters","DNMRange","Enter","FunctionNormalize","Normalize","RuntimeParseData","Skip","SpecialTagsOption","back_map","borrow","borrow","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","byte_offsets","chars","check","clone","clone","clone","clone_into","clone_into","clone_into","convert_to_lowercase","create_arange","default","default","default","deref","deref","deref","deref","deref","deref_mut","deref_mut","deref_mut","deref_mut","deref_mut","deserialize","dnm","drop","drop","drop","drop","drop","end","fmt","fmt","fmt","fmt","fmt","from","from","from","from","from","from_ams_paragraph_str","from_str","get_node","get_plaintext","get_plaintext","get_plaintext_truncated","get_range","get_range_of_node","get_subrange","get_subrange_from_byte_offsets","had_whitespace","init","init","init","init","init","into","into","into","into","into","is_empty","llamapun_normalization","new","node","node_c14n_basic","node_hash_basic","node_map","normalize_unicode","normalize_white_spaces","parameters","plaintext","root_node","runtime","serialize","serialize_node","serialize_offset","special_tag_class_options","special_tag_name_options","start","stem_words_full","stem_words_once","support_back_mapping","to_c14n_basic","to_hash_basic","to_owned","to_owned","to_owned","trim","try_from","try_from","try_from","try_from","try_from","try_into","try_into","try_into","try_into","try_into","type_id","type_id","type_id","type_id","type_id","wrap_tokens","0","0","lexematize_math","word_tokenize_for_vec2doc","Dictionary","Ngrams","add_anchored_content","add_content","anchor","borrow","borrow","borrow_mut","borrow_mut","count","counts","default","default","deref","deref","deref_mut","deref_mut","distinct_count","drop","drop","fmt","from","from","get","init","init","insert","insert","into","into","map","n","new","record_words","sorted","sorted","try_from","try_from","try_into","try_into","type_id","type_id","window_size","DNMRangeIterator","ItemDNM","ItemDNMRange","RoNodeIterator","XPathFilteredIterator","borrow","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","borrow_mut","corpus","deref","deref","deref","deref","deref_mut","deref_mut","deref_mut","deref_mut","dnm","document","document","document","document","document","drop","drop","drop","drop","from","from","from","from","get_document","get_document","init","init","init","init","into","into","into","into","into_iter","into_iter","iter","next","next","range","to_sentences","to_sentences","try_from","try_from","try_from","try_from","try_into","try_into","try_into","try_into","type_id","type_id","type_id","type_id","word_and_punct_iter","word_and_punct_iter","word_iter","word_iter","Corpus","borrow","borrow_mut","catalog_with_parallel_walk","default","deref","deref_mut","dnm_parameters","drop","extension","from","html_parser","init","into","new","path","tokenizer","try_from","try_into","type_id","xml_parser","Document","borrow","borrow_mut","corpus","deref","deref_mut","dnm","dom","drop","extended_paragraph_iter","filter_iter","from","get_heading_nodes","get_math_nodes","get_paragraph_nodes","get_ref_nodes","get_xpath_node","get_xpath_nodes","heading_iter","init","into","new","paragraph_iter","path","sentence_iter","try_from","try_into","type_id","xpath_selector_iter","MarkerEnum","Match","Math","MathMarker","PatternFile","PatternMarker","Text","TextMarker","borrow","borrow","borrow","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","clone","clone","clone","clone","clone","clone_into","clone_into","clone_into","clone_into","clone_into","deref","deref","deref","deref","deref","deref","deref_mut","deref_mut","deref_mut","deref_mut","deref_mut","deref_mut","description","drop","drop","drop","drop","drop","drop","from","from","from","from","from","from","get_marker_list","init","init","init","init","init","init","into","into","into","into","into","into","load","marker","marker","marker","match_sentence","math_rule_names","math_rules","mtext_rule_names","mtext_rules","name","node","pos_rule_names","pos_rules","range","sequence_rule_names","sequence_rules","sub_matches","tags","to_owned","to_owned","to_owned","to_owned","to_owned","try_from","try_from","try_from","try_from","try_from","try_from","try_into","try_into","try_into","try_into","try_into","try_into","type_id","type_id","type_id","type_id","type_id","type_id","word_rule_names","word_rules","0","0","load","Tokenizer","abbreviations","borrow","borrow_mut","default","deref","deref_mut","drop","from","init","into","sentences","stopwords","try_from","try_into","type_id","words","words_and_punct","data_helpers","path_helpers","plot","test","token_model","LexicalOptions","ams_normalize_word_range","borrow","borrow_mut","default","deref","deref_mut","discard_case","discard_math","discard_punct","drop","from","heading_from_node_aux","init","into","invalid_for_english_latin","normalize_heading_title","try_from","try_into","type_id","path_to_words","plot_simple","RESOURCE_DOCUMENTS","borrow","borrow_mut","deref","deref","deref_mut","drop","from","init","into","try_from","try_into","type_id","extract"],"q":["llamapun","","","","","","","","","","","llamapun::ams","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun::data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun::dnm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun::dnm::SpecialTagsOption","","llamapun::dnm::node","llamapun::extern_use","llamapun::ngrams","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun::parallel_data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun::parallel_data::corpus","","","","","","","","","","","","","","","","","","","","","llamapun::parallel_data::document","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun::patterns","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun::patterns::MarkerEnum","","llamapun::stopwords","llamapun::tokenizer","","","","","","","","","","","","","","","","","","llamapun::util","","","","","llamapun::util::data_helpers","","","","","","","","","","","","","","","","","","","","llamapun::util::path_helpers","llamapun::util::plot","llamapun::util::test","","","","","","","","","","","","","llamapun::util::token_model"],"d":["Representation, normalization and utilities for working …","Data structures and Iterators for convenient high-level …","The <code>dnm</code> can be used for easier switching between the DOM …","Expose convenience calls to be used from non-Rust …","A small ngram library ngrams are sequences of n …","Data structures and Iterators for rayon-enabled parallel …","A module for pattern matching in mathematical documents","A handy macro for idiomatic recording in the node_map","A tiny stopwords library Stopwords are words frequent …","Provides functionality for tokenizing sentences and words","Various useful code snippets","","","typically co-author support for a proof/paper (also “…","To be analyzed (?)","usually defines a computer science algorithm (also “…","Author-annotated \\\\newthorem{} environments using the …","","To be analyzed (?)","","","assumption/axiom/assertion/prior – should they be …","","To be analyzed (?)","","usually an actual Figure or Table captions realized via …","","A case in a multi-step proof / description / exposition","","To be analyzed (?)","To be analyzed (?)","","To be analyzed (?)","","Potentially a constraint on a proof","","An unproven statement/theorem (includes “conjecture”, …","To be analyzed (?)","","To be analyzed (?)","","A direct-to-derive consequence of a prior proposition","To be analyzed (?)","","","","Unlike notations, introduces new conceptual mathematical …","","To be analyzed (?)","","","To be analyzed (?)","","Demonstration of a definition, notation etc (also “…","To be analyzed (?)","To be analyzed (?)","","To be analyzed (?)","To be analyzed (?)","","To be analyzed (?)","","To be analyzed (?)","","","To be analyzed (?)","To be analyzed (?)","","A smaller sub-theorem to a main theorem","","","","","Introduces a new syntactic rule, usually for convenience / …","To be analyzed (?)","To be analyzed (?)","","To be analyzed (?)","","Anything else that was marked up with AMS, but doesn’t …","A named paragraph, without a clear standalone function","","To be analyzed (?)","A task to be solved (sometimes with solution following), …","A task to be solved (sometimes with solution following), …","","Proves a prior theorem/lemma","","","A provably true/false statement. Is this a synonym to …","","(sometimes) initial goal of inquiry (also “puzzle”, “…","","","A comment that is an aside to the main line of reasoning","","Summarizes paper’s experimental deliverables","To be analyzed (?)","","To be analyzed (?)","","A part of a proof, or demonstration/layout","Semantically fixed structural environments in scientific …","","To be analyzed (?)","","A main proposition to be proven in the document","","","","","","Maps a latexml-produced HTML class, such as “ltx_theorem …","","","","","","","","","","","","","","","","","","","","Checks a llamapun <code>Document</code> for ‘ltx_theorem’ AMS markup","Checks a libxml document for <code>ltx_theorem</code> AMS markup","","","","","If known, maps a commonly used AMS environment to a …","","","","","","","","","","","An iterable Corpus of HTML5 documents","One of our math documents.","File-system iterator yielding individual documents","A paragraph of a document with a DNM","An iterator over paragraphs of a <code>Document</code>. Ignores …","An iterator over the words of a sentence, where the words …","A sentence in a document","An iterator over the sentences of a document/paragraph","An iterator over the words of a sentence, where the words …","A word with a POS tag","","","","","","","","","","","","","","","","","","","","","reference to the parent corpus","A reference to the corpus containing this document","","","","","","","","","","","","","","","","","","","","","","If it exists, the DNM corresponding to this document","The dnm of this paragraph","Default setting for <code>DNM</code> generation","A reference to the document over which we iterate","A reference to the document containing this paragraph","A reference to the document we are working on","The document containing this sentence","The DOM of the document","","","","","","","","","","","Extension of corpus files (for specially tailored …","","","","","","","","","","","Obtain the MathML  nodes of a libxml <code>Document</code>","Obtain the &lt;span[class=ltx_ref]&gt; nodes of a libxml <code>Document</code>","document HTML5 parser","","","","","","","","","","","","","","","","","","","","","","","","","","Get an iterator over the documents","Get an iterator over the sentences in this paragraph","Load a specific document in the corpus","Create a new corpus with the base directory <code>dirpath</code>","Load a new document","","","","","","Get an iterator over the paragraphs of the document","Obtain the problem-free logical paragraphs of a libxml …","root directory","The file path of the document","The part-of-speech tag of the word (or POS::NOT_SET)","The range of the sentence","The range of the word","<code>Senna</code> object for shallow language analysis","Get an iterator over the words using Senna","<code>Senna</code> parsing options","Parses the sentence using Senna. The parse options are set …","If it exists, also the senna version of the sentence, …","The sentence containing the words","The sentence we are iterating over","The sentence containing this word","Get an iterator over the sentences of the document","Get an iterator over the words (using rudimentary …","<code>DNM</code>-aware sentence and word tokenizer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","document XHTML5 parser","The <code>DNM</code> is essentially a wrapper around the plain text …","Parameters for the DNM generation","Very often we’ll talk about substrings of the plaintext …","Recurse into tag (default behaviour)","Normalize tag, obtain replacement string by function call","Normalize tag, replacing it by some token","Some temporary data for the parser","Skip tag","Specifies how to deal with a certain tag","maps an offset to the corresponding node, and the offset …","","","","","","","","","","","As the plaintext is UTF-8: the byte offsets of the …","plaintext representation as vector of chars (to deal with …","Prints warnings, if the parameter settings don’t make …","","","","","","","Move to lowercase (remark: The stemmer does this …","creates an arange from to xpointers","","","Don’t do anything fancy and specific by default","","","","","","","","","","","deserializes an xpointer into a <code>DNMRange</code>. Note that only a …","DNM containing this range","","","","","","Offset of the end of the range","","","","","","","","","","","Rebuild a llamapun-generated tokenized plaintext into a DNM","Use the DNM abstraction over a plaintext utterance, …","Get the first corresponding DOM node for this range","Get the underlying text for this DNM","Get the plaintext substring corresponding to the range","Get the plaintext without trailing white spaces","Get the range representing the full DNM","Get the plaintext range of a node","returns a subrange, with offsets relative to the beginning …","returns a subrange from a pair of byte offsets (not …","plaintext is currently terminated by some whitespace","","","","","","","","","","","checks whether the range is empty","Normalize in a reasonable way for our math documents","Creates a <code>DNM</code> for <code>root</code>","Node auxiliaries for DNMs","Canonicalize a single node of choice","Obtain an MD5 hash from the canonical string of a Node","Maps nodes to plaintext offsets","Replace unicode characters by the ascii code representation","merge sequences of whitespaces into a single ’ ’. <em>Doesn</em>…","The options for generation","The plaintext","The root node of the underlying xml tree","A runtime object used for holding auxiliary state","serializes a DNMRange into an XPointer","serializes a node into an xpath expression","Serializes a node and an offset into an xpointer is_end …","How to deal with tags with special class names (e.g. …","How to deal with special tags (e.g. <code>&lt;math&gt;</code> tags)","Offset of the beginning of the range","Apply the morpha stemmer to the text nodes as often as it …","Apply the morpha stemmer once to the text nodes","Support back mapping, i.e. mapping plaintext offsets back …","Our linguistic canonical form will only include 1) node …","Obtain an MD5 hash from the canonical string of the entire …","","","","Returns a <code>DNMRange</code> with the leading and trailing …","","","","","","","","","","","","","","","","put spaces before and after tokens","","","Map math nodes to their lexemes","Interface function for vec2doc-expected word tokenization …","Records single words, in order of appearance","Ngrams are dictionaries with","In essence, for a given window size W, a word at index i …","add content for ngram analysis, typically a paragraph or a …","anchor word that must be present in all ngram contexts (in …","","","","","get the number of entries in the dictionary","statistics hashmap for the occurence counts","","","","","","","get the number of distinct ngrams recorded","","","","","","Get the word count","","","count a newly seen ngram phrase","insert a new word into the dictionary (if it hasn’t been …","","","hashmap for the records","n-grams for a sequence of n words","create a new dictionary","Take an arbitrarily long vector of words, and record all …","obtain the ngram report, sorted by descending frequency","get the entries of the dictionary sorted by occurence","","","","","","","if an anchor word is given, word window size, applied to …","A generic iterator over DNMRanges with their associated …","A DNM with associated document parent (e.g. for …","A DNMRange with associated document","Generic iterater over read-only xml nodes. It is the …","An iterator adaptor for filtered selections over a document","","","","","","","","","container and API for a Corpus capable of parallel walks …","","","","","","","","","The payload of the item","container and API for a Document yielded during a parallel …","A reference to the parent document","The document containing this sentence","A reference to the owner document","A reference to the document we are working on","","","","","","","","","the owner document being selected over","","","","","","","","","","","","Get an iterator over the sentences in this paragraph","","","The range of the sentence","the sentences for the resulting selection","","","","","","","","","","","","","","Get an iterator over the words and punctuation (using …","Get an iterator over the words and punctuation (using …","Get an iterator over the words (using rudimentary …","Get an iterator over the words (using rudimentary …","A parallel iterable Corpus of HTML5 documents","","","Get a parallel iterator over the documents","","","","Default setting for <code>DNM</code> generation","","Extension of corpus files (for specially tailored …","","document HTML5 parser","","","Create a new parallel-processing corpus with the base …","root directory","<code>DNM</code>-aware sentence and word tokenizer","","","","document XHTML5 parser","One of our math documents, thread-friendly","","","A reference to the corpus containing this document","","","If it exists, the DNM corresponding to this document","The DOM of the document","","Get an iterator over textual paragraphs of the document, …","Get an iterator using a custom closure predicate filter …","","Obtain the problem-free logical headings of a libxml …","Obtain the MathML  nodes of a libxml <code>Document</code>","Obtain the problem-free logical paragraphs of a libxml …","Obtain the &lt;span[class=ltx_ref]&gt; nodes of a libxml <code>Document</code>","Obtain the first node associated with the xpath evaluation …","Obtain the nodes associated with the xpath evaluation over …","Get an iterator over the headings of the document","","","Load a new document","Get an iterator over the paragraphs of the document","The file path of the document","Get an iterator over the sentences of the document","","","","Get an iterator over a custom xpath selector over the …","Any marked result","A <code>Match</code>. Note that matches are represented in a tree …","a marked math node","A marked math node","Contains rules loaded from a pattern file","The marker used for marking patterns. If a match was …","a marked text range","A marked text range","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","description of the file","","","","","","","","","","","","","returns a list of all markers","","","","","","","","","","","","","loads a pattern file","The marker associated with this match","the marker","the marker","returns the matches in a sentence","matches names of math rules to their offsets","the math rules","matches names of mtext rules to their offsets","the mtext rules (math symbols)","name of the marker","the marked math node","matches names of POS rules to their offsets","the POS rules","the marked range","matches names of sequence rules to their offsets","the sequence rules","The sub-matches","tags of the marker","","","","","","","","","","","","","","","","","","","","","","","","matches names of word rules to their offsets","the word rules","","","Load a set of stopwords Annoyingly, <code>HashSet</code>s are not …","Stores auxiliary resources required by the tokenizer so …","regular expression for abbreviations","","","","","","","","","","gets the sentences from a dnm","set of stopwords","","","","returns the words of a sentence using simple heuristics","returns the words and punctuation of a sentence, using …","Helpers with transactional logic related to llamapun::data …","Helpers intended mostly for non-Rust use, where rust is …","Some plotting functionality using gnuplot","Test utilities for llamapun’s crate","A “corpus token model”-generation utilities","Options for lexical normalization on an individual word","Normalization of word lexemes created for the “AMS …","","","","","","all letters will be lowercased when set","math will be entirely omitted when set","non-alphanumeric characters will be entirely omitted when …","","","Provides a string for a given heading node, using …","","","Check if the given DNM contains valid English+Latin content","Attempt to recover the “type” of a potentially …","","","","Given a path to a document, return a word-tokenized string …","A simple plot","shorthand global for all usable documents in the …","","","","","","","","","","","","","Parallel traversal of latexml-style HTML5 document …"],"i":[0,0,0,0,0,0,0,0,0,0,0,1,1,2,2,2,0,1,2,1,1,2,1,2,1,2,1,2,1,2,2,1,2,1,2,1,2,2,1,2,1,2,2,1,1,1,2,1,2,1,1,2,1,2,2,2,1,2,2,1,2,1,2,1,1,2,2,1,2,1,1,1,1,2,2,2,1,2,1,2,2,1,2,1,2,1,2,1,1,2,1,2,1,1,2,1,2,2,1,2,1,2,0,1,2,1,2,1,1,2,1,2,0,1,2,1,2,1,2,1,2,1,2,1,2,1,1,2,2,1,1,2,0,0,1,2,1,2,0,1,2,1,2,1,2,1,2,1,2,0,0,0,0,0,0,0,0,0,0,3,4,5,6,7,8,9,10,11,12,3,4,5,6,7,8,9,10,11,12,4,5,3,3,4,5,6,7,8,9,10,11,12,3,4,5,6,7,8,9,10,11,12,5,7,3,6,7,8,9,5,3,4,5,6,7,8,9,10,11,12,3,3,4,5,6,7,8,9,10,11,12,5,5,3,3,4,5,6,7,8,9,10,11,12,3,4,5,6,7,8,9,10,11,12,4,6,8,10,11,3,7,3,3,5,4,6,8,10,11,5,5,3,5,12,9,12,3,9,3,9,9,10,11,12,5,9,3,3,4,5,6,7,8,9,10,11,12,3,4,5,6,7,8,9,10,11,12,3,4,5,6,7,8,9,10,11,12,3,0,0,0,13,13,13,0,13,0,14,14,15,13,16,17,14,15,13,16,17,14,15,16,13,16,17,13,16,17,16,17,14,15,16,14,15,13,16,17,14,15,13,16,17,17,17,14,15,13,16,17,17,14,15,13,16,17,14,15,13,16,17,14,14,17,14,17,17,14,14,17,17,15,14,15,13,16,17,14,15,13,16,17,17,16,14,0,14,14,14,16,16,14,14,14,14,17,17,17,16,16,17,16,16,16,14,14,13,16,17,17,14,15,13,16,17,14,15,13,16,17,14,15,13,16,17,16,18,19,0,0,0,0,20,20,20,20,21,20,21,21,20,20,21,20,21,20,21,20,20,21,21,20,21,20,20,21,20,21,20,21,21,20,21,20,20,21,20,21,20,21,20,21,20,0,0,0,0,0,22,23,24,25,22,23,24,25,0,22,23,24,25,22,23,24,25,22,0,22,23,24,25,22,23,24,25,22,23,24,25,26,22,22,23,24,25,22,23,24,25,24,25,26,24,25,23,26,22,22,23,24,25,22,23,24,25,22,23,24,25,22,23,22,23,0,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,0,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,0,0,29,0,0,0,29,0,30,31,32,33,34,29,30,31,32,33,34,29,31,32,33,34,29,31,32,33,34,29,30,31,32,33,34,29,30,31,32,33,34,29,30,30,31,32,33,34,29,30,31,32,33,34,29,31,30,31,32,33,34,29,30,31,32,33,34,29,30,31,33,34,0,30,30,30,30,32,33,30,30,34,30,30,31,32,31,32,33,34,29,30,31,32,33,34,29,30,31,32,33,34,29,30,31,32,33,34,29,30,30,35,36,0,0,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,0,0,0,0,0,0,0,38,38,38,38,38,38,38,38,38,38,0,38,38,0,0,38,38,38,0,0,0,39,39,39,39,39,39,39,39,39,39,39,39,0],"f":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,[[]],[[]],[[]],[[]],[[["str",15]],[["amsenv",4],["option",4,["amsenv"]]]],[[],["structuralenv",4]],[[],["amsenv",4]],[[]],[[]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["structuralenv",4]],["bool",15]],[[["amsenv",4]],["bool",15]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[]],[[["str",15]],["structuralenv",4]],[[]],[[["document",3]],["bool",15]],[[["xmldoc",3]],["bool",15]],[[],["usize",15]],[[],["usize",15]],[[]],[[]],[[["str",15]],["amsenv",4]],[[]],[[]],[[],["string",3]],[[],["string",3]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],null,null,null,null,null,null,null,null,null,null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],null,null,[[],["corpus",3]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],null,null,null,null,null,null,null,null,[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],[["vec",3,["ronode"]],["ronode",3]]],[[],[["vec",3,["ronode"]],["ronode",3]]],null,[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["documentiterator",3]],[[],["sentenceiterator",3]],[[["string",3]],[["xmlparseerror",4],["result",4,["document","xmlparseerror"]],["document",3]]],[[["string",3]]],[[["string",3],["corpus",3]],[["result",4,["xmlparseerror"]],["xmlparseerror",4]]],[[],[["option",4,["document"]],["document",3]]],[[],[["option",4,["paragraph"]],["paragraph",3]]],[[],[["option",4,["sentence"]],["sentence",3]]],[[],[["word",3],["option",4,["word"]]]],[[],[["word",3],["option",4,["word"]]]],[[],["paragraphiterator",3]],[[["xmldoc",3]],[["vec",3,["ronode"]],["ronode",3]]],null,null,null,null,null,null,[[],["sennaworditerator",3]],null,[[]],null,null,null,null,[[],["sentenceiterator",3]],[[],["simpleworditerator",3]],null,[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],null,null,null,null,null,null,null,null,null,null,null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],null,null,[[]],[[],["specialtagsoption",4]],[[],["dnmparameters",3]],[[],["dnmrange",3]],[[]],[[]],[[]],null,[[["str",15]],["string",3]],[[],["dnm",3]],[[],["runtimeparsedata",3]],[[],["dnmparameters",3]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["str",15],["context",3],["dnm",3]],["dnmrange",3]],null,[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],null,[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[]],[[]],[[]],[[]],[[]],[[["str",15],["option",4,["dnmparameters"]],["dnmparameters",3]],[["box",3,["error"]],["result",4,["box"]]]],[[["str",15],["option",4,["dnmparameters"]],["dnmparameters",3]],[["box",3,["error"]],["result",4,["box"]]]],[[],["ronode",3]],[[],["str",15]],[[],["str",15]],[[],["str",15]],[[],[["box",3,["error"]],["result",4,["dnmrange","box"]],["dnmrange",3]]],[[["ronode",3]],[["box",3,["error"]],["result",4,["dnmrange","box"]],["dnmrange",3]]],[[["usize",15]],["dnmrange",3]],[[["usize",15]],["dnmrange",3]],null,[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[]],[[]],[[]],[[]],[[]],[[],["bool",15]],[[],["dnmparameters",3]],[[["ronode",3],["dnmparameters",3]],["dnm",3]],null,[[["ronode",3]],["string",3]],[[["ronode",3]],["string",3]],null,null,null,null,null,null,null,[[],["string",3]],[[["ronode",3],["bool",15]],["string",3]],[[["ronode",3],["i32",15],["bool",15]],["string",3]],null,null,null,null,null,null,[[],["string",3]],[[],["string",3]],[[]],[[]],[[]],[[],["dnmrange",3]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],null,null,null,[[["ronode",3],["context",3]],["string",3]],[[]],null,null,[[["str",15]]],[[["str",15]]],null,[[]],[[]],[[]],[[]],[[],["usize",15]],null,[[],["ngrams",3]],[[],["dictionary",3]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["formatter",3]],["result",6]],[[]],[[]],[[["str",15]],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[["string",3]]],[[["string",3]]],[[]],[[]],null,null,[[]],[[["str",15],["vec",3,["str"]]]],[[],["vec",3]],[[],["vec",3]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],null,null,null,null,null,null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],null,[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],null,null,null,null,null,null,[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["document",3]],[[],["document",3]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["dnmrangeiterator",3]],[[],[["itemdnm",3],["option",4,["itemdnm"]]]],[[],[["option",4,["itemdnmrange"]],["itemdnmrange",3]]],null,[[],[["vec",3,["dnmrange"]],["dnmrange",3]]],[[],[["vec",3,["dnmrange"]],["dnmrange",3]]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["dnmrangeiterator",3]],[[],["dnmrangeiterator",3]],[[],["dnmrangeiterator",3]],[[],["dnmrangeiterator",3]],null,[[]],[[]],[[],[["hashmap",3,["string","u64"]],["string",3],["u64",15]]],[[],["corpus",3]],[[["usize",15]]],[[["usize",15]]],null,[[["usize",15]]],null,[[]],null,[[],["usize",15]],[[]],[[["string",3]]],null,null,[[],["result",4]],[[],["result",4]],[[],["typeid",3]],null,null,[[]],[[]],null,[[["usize",15]]],[[["usize",15]]],null,null,[[["usize",15]]],[[],["ronodeiterator",3]],[[["fn",8]],["ronodeiterator",3]],[[]],[[],[["vec",3,["ronode"]],["ronode",3]]],[[],[["vec",3,["ronode"]],["ronode",3]]],[[],[["vec",3,["ronode"]],["ronode",3]]],[[],[["vec",3,["ronode"]],["ronode",3]]],[[["str",15]],[["ronode",3],["option",4,["ronode"]]]],[[["str",15]],[["vec",3,["ronode"]],["ronode",3]]],[[],["ronodeiterator",3]],[[],["usize",15]],[[]],[[["string",3],["corpus",3]],[["result",4,["xmlparseerror"]],["xmlparseerror",4]]],[[],["ronodeiterator",3]],null,[[],["dnmrangeiterator",3]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[["str",15]],["ronodeiterator",3]],null,null,null,null,null,null,null,null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["match",3]],[[],["patternmarker",3]],[[],["mathmarker",3]],[[],["textmarker",3]],[[],["markerenum",4]],[[]],[[]],[[]],[[]],[[]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],null,[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[],[["vec",3,["markerenum"]],["markerenum",4]]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[],["usize",15]],[[]],[[]],[[]],[[]],[[]],[[]],[[["str",15]],[["string",3],["patternfile",3],["result",4,["patternfile","string"]]]],null,null,null,[[["dnmrange",3],["str",15],["sentence",3],["patternfile",3]],[["result",4,["vec","string"]],["vec",3,["match"]],["string",3]]],null,null,null,null,null,null,null,null,null,null,null,null,null,[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],null,null,null,null,[[],[["hashset",3,["str"]],["str",15]]],null,null,[[]],[[]],[[],["tokenizer",3]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[],["usize",15]],[[]],[[["dnm",3]],[["dnmrange",3],["vec",3,["dnmrange"]]]],null,[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[["dnmrange",3]],[["dnmrange",3],["vec",3,["dnmrange"]]]],[[["dnmrange",3]],[["dnmrange",3],["vec",3,["dnmrange"]]]],null,null,null,null,null,null,[[["dnmrange",3],["lexicaloptions",3],["context",3]],[["string",3],["box",3,["error"]],["result",4,["string","box"]]]],[[]],[[]],[[]],[[["usize",15]]],[[["usize",15]]],null,null,null,[[["usize",15]]],[[]],[[["ronode",3],["tokenizer",3],["context",3]],[["string",3],["option",4,["string"]]]],[[],["usize",15]],[[]],[[["dnm",3]],["bool",15]],[[["str",15]],["string",3]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[["string",3]],["string",3]],[[["str",15]]],null,[[]],[[]],[[["usize",15]]],[[],["vec",3]],[[["usize",15]]],[[["usize",15]]],[[]],[[],["usize",15]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[["string",3],["bool",15]],[["hashmap",3,["string","u64"]],["result",4,["hashmap","box"]],["box",3,["error"]]]]],"p":[[4,"StructuralEnv"],[4,"AmsEnv"],[3,"Corpus"],[3,"DocumentIterator"],[3,"Document"],[3,"ParagraphIterator"],[3,"Paragraph"],[3,"SentenceIterator"],[3,"Sentence"],[3,"SimpleWordIterator"],[3,"SennaWordIterator"],[3,"Word"],[4,"SpecialTagsOption"],[3,"DNM"],[3,"RuntimeParseData"],[3,"DNMParameters"],[3,"DNMRange"],[13,"Normalize"],[13,"FunctionNormalize"],[3,"Ngrams"],[3,"Dictionary"],[3,"ItemDNM"],[3,"ItemDNMRange"],[3,"RoNodeIterator"],[3,"DNMRangeIterator"],[8,"XPathFilteredIterator"],[3,"Corpus"],[3,"Document"],[4,"MarkerEnum"],[3,"PatternFile"],[3,"Match"],[3,"PatternMarker"],[3,"MathMarker"],[3,"TextMarker"],[13,"Text"],[13,"Math"],[3,"Tokenizer"],[3,"LexicalOptions"],[3,"RESOURCE_DOCUMENTS"]]}\
}');
if (window.initSearch) {window.initSearch(searchIndex)};