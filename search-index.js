var searchIndex = JSON.parse('{\
"llamapun":{"doc":"The <code>LLaMaPUn</code> library in Rust","t":[0,0,3,12,12,12,5,5,5,5,0,5,0,5,0,3,0,5,0,5,5,4,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,4,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,5,5,0,3,12,12,12,12,12,12,12,12,3,12,3,12,12,12,12,3,12,3,12,12,3,12,3,12,12,12,3,12,3,12,3,12,12,12,11,11,11,11,11,11,11,11,11,11,11,11,11,0,0,5,3,12,12,12,12,12,12,12,12,12,3,12,12,4,13,13,13,13,3,12,12,12,3,12,12,12,12,12,12,12,11,11,11,11,11,11,0,3,12,11,11,11,11,3,12,12,12,12,11,11,11,11,11,11,11,0,0,3,12,12,12,12,12,12,11,11,0,3,12,12,12,12,11,11,11,11,11,11,11,11,11,11,11,11,11,3,12,12,3,12,12,3,12,3,12,8,10,10,11,11,11,11,11,0,5,3,12,12,4,13,13,3,12,12,3,12,12,12,12,12,12,12,12,12,12,12,3,12,12,3,12,12,0,5,0,3,12,12,11,11,11,0,5,14,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11],"n":["util","data_helpers","LexicalOptions","discard_math","discard_punct","discard_case","ams_normalize_word_range","heading_from_node_aux","normalize_heading_title","invalid_for_english_latin","path_helpers","path_to_words","plot","plot_simple","test","RESOURCE_DOCUMENTS","token_model","extract","ams","has_markup","has_markup_xmldoc","StructuralEnv","Abstract","Acknowledgement","Analysis","Application","Assumption","Background","Case","Caption","Claim","Conclusion","Condition","Conjecture","Contribution","Corollary","Data","Dataset","Definition","Demonstration","Description","Discussion","Example","Experiment","Fact","FutureWork","Implementation","Introduction","Lemma","Methods","Model","Motivation","Notation","Observation","Other","Preliminaries","Problem","Proof","Property","Proposition","Question","RelatedWork","Remark","Result","Simulation","Step","Summary","Theorem","Theory","AmsEnv","Acknowledgement","Algorithm","Answer","Affirmation","Assumption","Bound","Caption","Case","Claim","Comment","Conclusion","Condition","Conjecture","Constraint","Convention","Corollary","Criterion","Definition","Demonstration","Discussion","Example","Experiment","Expansion","Expectation","Explanation","Fact","Hint","Issue","Keywords","Lemma","Notation","Note","Notice","Observation","Paragraph","Principle","Problem","Proof","Proposition","Question","Remark","Result","Rule","Solution","Step","Summary","Theorem","Other","class_to_env","normalize_env","data","Corpus","path","xml_parser","html_parser","tokenizer","senna","senna_options","dnm_parameters","extension","DocumentIterator","corpus","Document","dom","path","corpus","dnm","ParagraphIterator","document","Paragraph","dnm","document","SentenceIterator","document","Sentence","range","document","senna_sentence","SimpleWordIterator","sentence","SennaWordIterator","sentence","Word","range","sentence","pos","new","iter","load_doc","new","paragraph_nodes","paragraph_iter","get_math_nodes","get_ref_nodes","sentence_iter","iter","simple_iter","senna_iter","senna_parse","dnm","node","lexematize_math","DNMParameters","special_tag_name_options","special_tag_class_options","normalize_white_spaces","wrap_tokens","normalize_unicode","stem_words_once","stem_words_full","convert_to_lowercase","support_back_mapping","RuntimeParseData","had_whitespace","chars","SpecialTagsOption","Enter","Normalize","FunctionNormalize","Skip","DNMRange","start","end","dnm","DNM","plaintext","byte_offsets","parameters","root_node","node_map","runtime","back_map","new","from_str","from_ams_paragraph_str","get_range_of_node","get_range","get_plaintext","ngrams","Dictionary","map","new","insert","sorted","count","Ngrams","anchor","window_size","n","counts","get","insert","sorted","distinct_count","add_content","add_anchored_content","record_words","parallel_data","corpus","Corpus","path","xml_parser","html_parser","tokenizer","dnm_parameters","extension","new","catalog_with_parallel_walk","document","Document","dom","path","corpus","dnm","new","get_heading_nodes","heading_iter","get_paragraph_nodes","paragraph_iter","extended_paragraph_iter","get_math_nodes","get_ref_nodes","sentence_iter","get_xpath_nodes","get_xpath_node","xpath_selector_iter","filter_iter","ItemDNM","dnm","document","ItemDNMRange","range","document","RoNodeIterator","document","DNMRangeIterator","document","XPathFilteredIterator","to_sentences","get_document","iter","word_iter","word_and_punct_iter","word_iter","word_and_punct_iter","patterns","match_sentence","Match","marker","sub_matches","MarkerEnum","Text","Math","MathMarker","node","marker","PatternFile","description","word_rules","pos_rules","math_rules","mtext_rules","sequence_rules","word_rule_names","pos_rule_names","math_rule_names","mtext_rule_names","sequence_rule_names","PatternMarker","name","tags","TextMarker","range","marker","stopwords","load","tokenizer","Tokenizer","stopwords","abbreviations","sentences","words","words_and_punct","extern_use","word_tokenize_for_vec2doc","record_node_map","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","to_owned","clone_into","to_string","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","to_owned","clone_into","to_string","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","into_iter","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","into_iter","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","into_iter","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","into_iter","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","into_iter","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","to_owned","clone_into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","to_owned","clone_into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","to_owned","clone_into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","into_iter","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","into_iter","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","to_owned","clone_into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","to_owned","clone_into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","to_owned","clone_into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","to_owned","clone_into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","to_owned","clone_into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","from","into","borrow","borrow_mut","try_from","try_into","type_id","init","deref","deref_mut","drop","get_document","to_sentences","from","next","next","next","next","next","next","next","clone","clone","clone","clone","clone","clone","clone","clone","clone","clone","default","default","default","default","default","default","default","default","default","eq","eq","deref","fmt","fmt","fmt","fmt","fmt","fmt","fmt","fmt","fmt","initialize","to_c14n_basic","node_c14n_basic","to_hash_basic","node_hash_basic","llamapun_normalization","check","get_plaintext","get_plaintext_truncated","get_node","trim","get_subrange","get_subrange_from_byte_offsets","is_empty","serialize","create_arange","serialize_offset","serialize_node","deserialize","get_marker_list","load"],"q":["llamapun","llamapun::util","llamapun::util::data_helpers","","","","","","","","llamapun::util","llamapun::util::path_helpers","llamapun::util","llamapun::util::plot","llamapun::util","llamapun::util::test","llamapun::util","llamapun::util::token_model","llamapun","llamapun::ams","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun","llamapun::data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun","llamapun::dnm","llamapun::dnm::node","llamapun::dnm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun","llamapun::ngrams","","","","","","","","","","","","","","","","","","llamapun","llamapun::parallel_data","llamapun::parallel_data::corpus","","","","","","","","","llamapun::parallel_data","llamapun::parallel_data::document","","","","","","","","","","","","","","","","","","llamapun::parallel_data","","","","","","","","","","","","","","","","","","llamapun","llamapun::patterns","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun","llamapun::stopwords","llamapun","llamapun::tokenizer","","","","","","llamapun","llamapun::extern_use","llamapun","llamapun::util::data_helpers","","","","","","","","","","","llamapun::util::test","","","","","","","","","","","llamapun::ams","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun::data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun::dnm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun::ngrams","","","","","","","","","","","","","","","","","","","","","","llamapun::parallel_data::corpus","","","","","","","","","","","llamapun::parallel_data::document","","","","","","","","","","","llamapun::parallel_data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun::patterns","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","llamapun::tokenizer","","","","","","","","","","","llamapun::parallel_data","","llamapun::ams","llamapun::data","","","","","llamapun::parallel_data","","llamapun::ams","","llamapun::dnm","","","llamapun::patterns","","","","","llamapun::util::data_helpers","llamapun::data","llamapun::dnm","","","llamapun::ngrams","","llamapun::parallel_data::corpus","llamapun::tokenizer","llamapun::ams","","llamapun::util::test","llamapun::ams","","llamapun::dnm","","","","","llamapun::ams","","llamapun::util::test","llamapun::dnm","","","","","","","","","","","","","","","","","","llamapun::patterns",""],"d":["Various useful code snippets","Helpers with transactional logic related to llamapun::data…","Options for lexical normalization on an individual word","math will be entirely omitted when set","non-alphanumeric characters will be entirely omitted when …","all letters will be lowercased when set","Normalization of word lexemes created for the “AMS …","Provides a string for a given heading node, using …","Attempt to recover the “type” of a potentially …","Check if the given DNM contains valid English+Latin …","Helpers intended mostly for non-Rust use, where rust is …","Given a path to a document, return a word-tokenized …","Some plotting functionality using gnuplot","A simple plot","Test utilities for llamapun’s crate","shorthand global for all usable documents in the …","A “corpus token model”-generation utilities","Parallel traversal of latexml-style HTML5 document …","Representation, normalization and utilities for working …","Checks a llamapun <code>Document</code> for ‘ltx_theorem’ AMS …","Checks a libxml document for <code>ltx_theorem</code> AMS markup","Semantically fixed structural environments in scientific …","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","A task to be solved (sometimes with solution following), …","","","","","","","","","","","","","Author-annotated \\\\newthorem{} environments using the …","typically co-author support for a proof/paper (also “…","usually defines a computer science algorithm (also “…","To be analyzed (?)","To be analyzed (?)","assumption/axiom/assertion/prior – should they be …","To be analyzed (?)","usually an actual Figure or Table captions realized via …","A case in a multi-step proof / description / exposition","To be analyzed (?)","To be analyzed (?)","To be analyzed (?)","Potentially a constraint on a proof","An unproven statement/theorem (includes “conjecture”, …","To be analyzed (?)","To be analyzed (?)","A direct-to-derive consequence of a prior proposition","To be analyzed (?)","Unlike notations, introduces new conceptual mathematical …","To be analyzed (?)","To be analyzed (?)","Demonstration of a definition, notation etc (also “…","To be analyzed (?)","To be analyzed (?)","To be analyzed (?)","To be analyzed (?)","To be analyzed (?)","To be analyzed (?)","To be analyzed (?)","To be analyzed (?)","A smaller sub-theorem to a main theorem","Introduces a new syntactic rule, usually for convenience …","To be analyzed (?)","To be analyzed (?)","To be analyzed (?)","A named paragraph, without a clear standalone function","To be analyzed (?)","A task to be solved (sometimes with solution following), …","Proves a prior theorem/lemma","A provably true/false statement. Is this a synonym to …","(sometimes) initial goal of inquiry (also “puzzle”, …","A comment that is an aside to the main line of reasoning","Summarizes paper’s experimental deliverables","To be analyzed (?)","To be analyzed (?)","A part of a proof, or demonstration/layout","To be analyzed (?)","A main proposition to be proven in the document","Anything else that was marked up with AMS, but doesn’t …","Maps a latexml-produced HTML class, such as “…","If known, maps a commonly used AMS environment to a …","Data structures and Iterators for convenient high-level …","An iterable Corpus of HTML5 documents","root directory","document XHTML5 parser","document HTML5 parser","<code>DNM</code>-aware sentence and word tokenizer","<code>Senna</code> object for shallow language analysis","<code>Senna</code> parsing options","Default setting for <code>DNM</code> generation","Extension of corpus files (for specially tailored …","File-system iterator yielding individual documents","reference to the parent corpus","One of our math documents.","The DOM of the document","The file path of the document","A reference to the corpus containing this document","If it exists, the DNM corresponding to this document","An iterator over paragraphs of a <code>Document</code>. Ignores …","A reference to the document over which we iterate","A paragraph of a document with a DNM","The dnm of this paragraph","A reference to the document containing this paragraph","An iterator over the sentences of a document/paragraph","A reference to the document we are working on","A sentence in a document","The range of the sentence","The document containing this sentence","If it exists, also the senna version of the sentence, …","An iterator over the words of a sentence, where the words …","The sentence containing the words","An iterator over the words of a sentence, where the words …","The sentence we are iterating over","A word with a POS tag","The range of the word","The sentence containing this word","The part-of-speech tag of the word (or POS::NOT_SET)","Create a new corpus with the base directory <code>dirpath</code>","Get an iterator over the documents","Load a specific document in the corpus","Load a new document","Obtain the problem-free logical paragraphs of a libxml …","Get an iterator over the paragraphs of the document","Obtain the MathML  nodes of a libxml <code>Document</code>","Obtain the <span[class=ltx_ref]> nodes of a libxml …","Get an iterator over the sentences of the document","Get an iterator over the sentences in this paragraph","Get an iterator over the words (using rudimentary …","Get an iterator over the words using Senna","Parses the sentence using Senna. The parse options are …","The <code>dnm</code> can be used for easier switching between the DOM …","Node auxiliaries for DNMs","Map math nodes to their lexemes","Parameters for the DNM generation","How to deal with special tags (e.g. <code><math></code> tags)","How to deal with tags with special class names (e.g. …","merge sequences of whitespaces into a single ’ ’. <em>…","put spaces before and after tokens","Replace unicode characters by the ascii code …","Apply the morpha stemmer once to the text nodes","Apply the morpha stemmer to the text nodes as often as it …","Move to lowercase (remark: The stemmer does this …","Support back mapping, i.e. mapping plaintext offsets back …","Some temporary data for the parser","plaintext is currently terminated by some whitespace","plaintext representation as vector of chars (to deal with …","Specifies how to deal with a certain tag","Recurse into tag (default behaviour)","Normalize tag, replacing it by some token","Normalize tag, obtain replacement string by function call","Skip tag","Very often we’ll talk about substrings of the plaintext …","Offset of the beginning of the range","Offset of the end of the range","DNM containing this range","The <code>DNM</code> is essentially a wrapper around the plain text …","The plaintext","As the plaintext is UTF-8: the byte offsets of the …","The options for generation","The root node of the underlying xml tree","Maps nodes to plaintext offsets","A runtime object used for holding auxiliary state","maps an offset to the corresponding node, and the offset …","Creates a <code>DNM</code> for <code>root</code>","Use the DNM abstraction over a plaintext utterance, …","Rebuild a llamapun-generated tokenized plaintext into a …","Get the plaintext range of a node","Get the range representing the full DNM","Get the underlying text for this DNM","A small ngram library ngrams are sequences of n …","Records single words, in order of appearance","hashmap for the records","create a new dictionary","insert a new word into the dictionary (if it hasn’t …","get the entries of the dictionary sorted by occurence","get the number of entries in the dictionary","Ngrams are dictionaries with","anchor word that must be present in all ngram contexts …","if an anchor word is given, word window size, applied to …","n-grams for a sequence of n words","statistics hashmap for the occurence counts","Get the word count","count a newly seen ngram phrase","obtain the ngram report, sorted by descending frequency","get the number of distinct ngrams recorded","add content for ngram analysis, typically a paragraph or …","In essence, for a given window size W, a word at index i …","Take an arbitrarily long vector of words, and record all …","Data structures and Iterators for rayon-enabled parallel …","container and API for a Corpus capable of parallel walks …","A parallel iterable Corpus of HTML5 documents","root directory","document XHTML5 parser","document HTML5 parser","<code>DNM</code>-aware sentence and word tokenizer","Default setting for <code>DNM</code> generation","Extension of corpus files (for specially tailored …","Create a new parallel-processing corpus with the base …","Get a parallel iterator over the documents","container and API for a Document yielded during a …","One of our math documents, thread-friendly","The DOM of the document","The file path of the document","A reference to the corpus containing this document","If it exists, the DNM corresponding to this document","Load a new document","Obtain the problem-free logical headings of a libxml …","Get an iterator over the headings of the document","Obtain the problem-free logical paragraphs of a libxml …","Get an iterator over the paragraphs of the document","Get an iterator over textual paragraphs of the document, …","Obtain the MathML  nodes of a libxml <code>Document</code>","Obtain the <span[class=ltx_ref]> nodes of a libxml …","Get an iterator over the sentences of the document","Obtain the nodes associated with the xpath evaluation …","Obtain the first node associated with the xpath …","Get an iterator over a custom xpath selector over the …","Get an iterator using a custom closure predicate filter …","A DNM with associated document parent (e.g. for …","The payload of the item","A reference to the parent document","A DNMRange with associated document","The range of the sentence","The document containing this sentence","Generic iterater over read-only xml nodes. It is the …","A reference to the owner document","A generic iterator over DNMRanges with their associated …","A reference to the document we are working on","An iterator adaptor for filtered selections over a …","the sentences for the resulting selection","the owner document being selected over","Get an iterator over the sentences in this paragraph","Get an iterator over the words (using rudimentary …","Get an iterator over the words and punctuation (using …","Get an iterator over the words (using rudimentary …","Get an iterator over the words and punctuation (using …","A module for pattern matching in mathematical documents","returns the matches in a sentence","A <code>Match</code>. Note that matches are represented in a tree …","The marker associated with this match","The sub-matches","Any marked result","a marked text range","a marked math node","A marked math node","the marked math node","the marker","Contains rules loaded from a pattern file","description of the file","the word rules","the POS rules","the math rules","the mtext rules (math symbols)","the sequence rules","matches names of word rules to their offsets","matches names of POS rules to their offsets","matches names of math rules to their offsets","matches names of mtext rules to their offsets","matches names of sequence rules to their offsets","The marker used for marking patterns. If a match was …","name of the marker","tags of the marker","A marked text range","the marked range","the marker","A tiny stopwords library Stopwords are words frequent …","Load a set of stopwords Annoyingly, <code>HashSet</code>s are not …","Provides functionality for tokenizing sentences and words","Stores auxiliary resources required by the tokenizer so …","set of stopwords","regular expression for abbreviations","gets the sentences from a dnm","returns the words of a sentence using simple heuristics","returns the words and punctuation of a sentence, using …","Expose convenience calls to be used from non-Rust …","Interface function for vec2doc-expected word tokenization …","A handy macro for idiomatic recording in the node_map","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Don’t do anything fancy and specific by default","","","","","","","","","","","","","","","","","","","Our linguistic canonical form will only include 1) node …","Canonicalize a single node of choice","Obtain an MD5 hash from the canonical string of the …","Obtain an MD5 hash from the canonical string of a Node","Normalize in a reasonable way for our math documents","Prints warnings, if the parameter settings don’t make …","Get the plaintext substring corresponding to the range","Get the plaintext without trailing white spaces","Get the first corresponding DOM node for this range","Returns a <code>DNMRange</code> with the leading and trailing …","returns a subrange, with offsets relative to the …","returns a subrange from a pair of byte offsets (not …","checks whether the range is empty","serializes a DNMRange into an XPointer","creates an arange from to xpointers","Serializes a node and an offset into an xpointer is_end …","serializes a node into an xpath expression","deserializes an xpointer into a <code>DNMRange</code>. Note that only …","returns a list of all markers","loads a pattern file"],"i":[0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,0,0,0,0,4,4,4,4,4,4,4,4,0,5,0,6,6,6,6,0,7,0,8,8,0,9,0,10,10,10,0,11,0,12,0,13,13,13,4,4,4,6,6,6,6,6,6,8,10,10,10,0,0,0,0,14,14,14,14,14,14,14,14,14,0,15,15,0,16,16,16,16,0,17,17,17,0,18,18,18,18,18,18,18,18,18,18,18,18,18,0,0,19,19,19,19,19,0,20,20,20,20,20,20,20,20,20,20,20,0,0,0,21,21,21,21,21,21,21,21,0,0,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,0,23,23,0,24,24,0,25,0,26,0,27,27,27,24,24,23,23,0,0,0,28,28,0,29,29,0,30,30,0,31,31,31,31,31,31,31,31,31,31,31,0,32,32,0,33,33,0,0,0,0,34,34,34,34,34,0,0,0,1,1,1,1,1,1,1,1,1,1,1,35,35,35,35,35,35,35,35,35,35,35,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,13,13,13,13,13,13,13,13,13,13,13,15,15,15,15,15,15,15,15,15,15,15,16,16,16,16,16,16,16,16,16,16,16,16,16,14,14,14,14,14,14,14,14,14,14,14,14,14,17,17,17,17,17,17,17,17,17,17,17,17,17,18,18,18,18,18,18,18,18,18,18,18,19,19,19,19,19,19,19,19,19,19,19,20,20,20,20,20,20,20,20,20,20,20,21,21,21,21,21,21,21,21,21,21,21,22,22,22,22,22,22,22,22,22,22,22,23,23,23,23,23,23,23,23,23,23,23,24,24,24,24,24,24,24,24,24,24,24,25,25,25,25,25,25,25,25,25,25,25,25,26,26,26,26,26,26,26,26,26,26,26,26,28,28,28,28,28,28,28,28,28,28,28,28,28,32,32,32,32,32,32,32,32,32,32,32,32,32,30,30,30,30,30,30,30,30,30,30,30,30,30,33,33,33,33,33,33,33,33,33,33,33,33,33,29,29,29,29,29,29,29,29,29,29,29,29,29,31,31,31,31,31,31,31,31,31,31,31,34,34,34,34,34,34,34,34,34,34,34,23,23,2,5,7,9,11,12,25,26,2,3,16,14,17,28,32,30,33,29,1,4,15,14,18,19,20,21,34,2,3,35,2,3,15,16,14,17,18,2,3,35,18,18,18,18,14,14,17,17,17,17,17,17,17,17,17,17,17,17,28,31],"f":[null,null,null,null,null,null,[[["context",3],["dnmrange",3],["lexicaloptions",3]],[["result",4],["string",3],["box",3]]],[[["ronode",3],["context",3],["tokenizer",3]],[["string",3],["option",4]]],[[["str",15]],["string",3]],[[["dnm",3]],["bool",15]],null,[[["string",3]],["string",3]],null,[[["str",15]]],null,null,null,[[["string",3],["bool",15]],[["result",4],["hashmap",3],["box",3]]],null,[[["document",3]],["bool",15]],[[["xmldoc",3]],["bool",15]],null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,[[["str",15]],[["option",4],["amsenv",4]]],[[["str",15]],["amsenv",4]],null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,[[["string",3]]],[[],["documentiterator",3]],[[["string",3]],[["document",3],["result",4],["xmlparseerror",4]]],[[["string",3],["corpus",3]],[["result",4],["xmlparseerror",4]]],[[["xmldoc",3]],[["ronode",3],["vec",3]]],[[],["paragraphiterator",3]],[[],[["ronode",3],["vec",3]]],[[],[["ronode",3],["vec",3]]],[[],["sentenceiterator",3]],[[],["sentenceiterator",3]],[[],["simpleworditerator",3]],[[],["sennaworditerator",3]],[[]],null,null,[[["ronode",3],["context",3]],["string",3]],null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,[[["ronode",3],["dnmparameters",3]],["dnm",3]],[[["dnmparameters",3],["option",4],["str",15]],[["box",3],["result",4]]],[[["dnmparameters",3],["option",4],["str",15]],[["box",3],["result",4]]],[[["ronode",3]],[["box",3],["result",4],["dnmrange",3]]],[[],[["box",3],["result",4],["dnmrange",3]]],[[],["str",15]],null,null,null,[[]],[[["string",3]]],[[],["vec",3]],[[],["usize",15]],null,null,null,null,null,[[["str",15]],["usize",15]],[[["string",3]]],[[],["vec",3]],[[],["usize",15]],[[["str",15]]],[[["str",15]]],[[["vec",3],["str",15]]],null,null,null,null,null,null,null,null,null,[[["string",3]]],[[],[["string",3],["hashmap",3],["u64",15]]],null,null,null,null,null,null,[[["string",3],["corpus",3]],[["result",4],["xmlparseerror",4]]],[[],[["ronode",3],["vec",3]]],[[],["ronodeiterator",3]],[[],[["ronode",3],["vec",3]]],[[],["ronodeiterator",3]],[[],["ronodeiterator",3]],[[],[["ronode",3],["vec",3]]],[[],[["ronode",3],["vec",3]]],[[],["dnmrangeiterator",3]],[[["str",15]],[["ronode",3],["vec",3]]],[[["str",15]],[["ronode",3],["option",4]]],[[["str",15]],["ronodeiterator",3]],[[["fn",8]],["ronodeiterator",3]],null,null,null,null,null,null,null,null,null,null,null,[[],[["dnmrange",3],["vec",3]]],[[],["document",3]],[[],["dnmrangeiterator",3]],[[],["dnmrangeiterator",3]],[[],["dnmrangeiterator",3]],[[],["dnmrangeiterator",3]],[[],["dnmrangeiterator",3]],null,[[["str",15],["sentence",3],["patternfile",3],["dnmrange",3]],[["vec",3],["string",3],["result",4]]],null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,[[],[["str",15],["hashset",3]]],null,null,null,null,[[["dnm",3]],[["vec",3],["dnmrange",3]]],[[["dnmrange",3]],[["dnmrange",3],["vec",3]]],[[["dnmrange",3]],[["dnmrange",3],["vec",3]]],null,[[]],null,[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["string",3]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["string",3]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["usize",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[],["document",3]],[[],[["dnmrange",3],["vec",3]]],[[["str",15]],["structuralenv",4]],[[],[["option",4],["document",3]]],[[],[["paragraph",3],["option",4]]],[[],[["option",4],["sentence",3]]],[[],[["option",4],["word",3]]],[[],[["option",4],["word",3]]],[[],[["option",4],["itemdnm",3]]],[[],[["option",4],["itemdnmrange",3]]],[[],["structuralenv",4]],[[],["amsenv",4]],[[],["specialtagsoption",4]],[[],["dnmparameters",3]],[[],["dnmrange",3]],[[],["match",3]],[[],["patternmarker",3]],[[],["mathmarker",3]],[[],["textmarker",3]],[[],["markerenum",4]],[[]],[[],["corpus",3]],[[],["runtimeparsedata",3]],[[],["dnmparameters",3]],[[],["dnm",3]],[[],["dictionary",3]],[[],["ngrams",3]],[[],["corpus",3]],[[],["tokenizer",3]],[[["structuralenv",4]],["bool",15]],[[["amsenv",4]],["bool",15]],[[],["vec",3]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[]],[[],["string",3]],[[["ronode",3]],["string",3]],[[],["string",3]],[[["ronode",3]],["string",3]],[[],["dnmparameters",3]],[[]],[[],["str",15]],[[],["str",15]],[[],["ronode",3]],[[],["dnmrange",3]],[[["usize",15]],["dnmrange",3]],[[["usize",15]],["dnmrange",3]],[[],["bool",15]],[[],["string",3]],[[["str",15]],["string",3]],[[["ronode",3],["bool",15],["i32",15]],["string",3]],[[["ronode",3],["bool",15]],["string",3]],[[["dnm",3],["context",3],["str",15]],["dnmrange",3]],[[],[["vec",3],["markerenum",4]]],[[["str",15]],[["result",4],["string",3],["patternfile",3]]]],"p":[[3,"LexicalOptions"],[4,"StructuralEnv"],[4,"AmsEnv"],[3,"Corpus"],[3,"DocumentIterator"],[3,"Document"],[3,"ParagraphIterator"],[3,"Paragraph"],[3,"SentenceIterator"],[3,"Sentence"],[3,"SimpleWordIterator"],[3,"SennaWordIterator"],[3,"Word"],[3,"DNMParameters"],[3,"RuntimeParseData"],[4,"SpecialTagsOption"],[3,"DNMRange"],[3,"DNM"],[3,"Dictionary"],[3,"Ngrams"],[3,"Corpus"],[3,"Document"],[3,"ItemDNM"],[3,"ItemDNMRange"],[3,"RoNodeIterator"],[3,"DNMRangeIterator"],[8,"XPathFilteredIterator"],[3,"Match"],[4,"MarkerEnum"],[3,"MathMarker"],[3,"PatternFile"],[3,"PatternMarker"],[3,"TextMarker"],[3,"Tokenizer"],[3,"RESOURCE_DOCUMENTS"]]}\
}');
initSearch(searchIndex);